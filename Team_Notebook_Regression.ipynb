{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "PBui98TF65fp",
        "Ndvm2K_C7n1L",
        "w6K7xa23Elo4",
        "Nz4OGuKa9CyS",
        "mDgbUHAGgjLW",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "35m5QtbWiB9F",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "iDc6HHFMKrNf",
        "02CXGwxuc-oB",
        "sjLSPLJRm81j",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "jq-YJGHDp576",
        "YW8aNWAQp579",
        "yuFsdoKmp57-",
        "iqoKbMRap58A",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "pVOZrr_IlUWH",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "qvMRdAjP-f4_",
        "GxMRQFVa-f5B",
        "aUNNTdN6-f5Q",
        "_nsKino0-f5T",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabhsingh3786/Capstone-2-Retail-Sales-Prediction-/blob/main/Team_Notebook_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# <b><u> Project Name :- Sales Prediction : Predicting sales of a major store chain Rossmann</u></b>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PBui98TF65fp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Project Type**    - Regression\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 -** Bharathwaj Bejjarapu\n",
        "##### **Team Member 2 -** Saurabh Singh\n",
        "##### **Team Member 3 -** Shriya Chouhan\n"
      ],
      "metadata": {
        "id": "AbGMSI2o65fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "Ndvm2K_C7n1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.\n",
        "\n",
        "### We are provided with historical sales data for 1,115 Rossmann stores. The task is to forecast the \"Sales\" column for the test set. Note that some stores in the dataset were temporarily closed for refurbishment."
      ],
      "metadata": {
        "id": "QxROILT57n1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/saurabhsingh3786/Capstone-2-Retail-Sales-Prediction-\n",
        "\n",
        " https://github.com/bharath977/Rossmann_Retail_price_predication    \n",
        " https://github.com/ShriyaChouhan/Retail_Sales_Prediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "Nz4OGuKa9CyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to Develop a supervised machine learning model to accurately forecast the daily sales of Rossmann stores. Utilize historical sales data, along with additional features such as promotions, competition, holidays, seasonality, and locality, to predict the future sales for a given store. The goal is to provide reliable sales predictions that can assist store managers in making informed decisions, optimizing inventory management, and improving overall business performance.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xaHn3vQ99CyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import scipy.stats as stats\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import r2_score as r2, mean_squared_error as mse, mean_absolute_error as mae\n",
        "import math\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount = True)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/AlmaBetter/Capstone Projects/Retail Sales Prediction/\"\n",
        "rossmann_sales_df = pd.read_csv(path + \"Rossmann Stores Data.csv\", low_memory=False)\n",
        "stores_df = pd.read_csv(path + \"store.csv\")"
      ],
      "metadata": {
        "id": "rRL4zIiMAC7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "#Rossmann sales df first five rows view\n",
        "rossmann_sales_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rossmann sales df last five rows view\n",
        "rossmann_sales_df.tail()"
      ],
      "metadata": {
        "id": "fp-4jivwBU_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stores df first five rows view\n",
        "stores_df.head().T"
      ],
      "metadata": {
        "id": "HWiS4ojwBfcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stores df last five rows view\n",
        "stores_df.tail().T"
      ],
      "metadata": {
        "id": "zuJc3CGnCGuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "# Count the rows and columns in the Rossmann sales dataset\n",
        "sales_rows, sales_cols = rossmann_sales_df.shape\n",
        "\n",
        "# Count the rows and columns in the stores dataset\n",
        "stores_rows, stores_cols = stores_df.shape\n",
        "\n",
        "# Print the results\n",
        "print(f\"Rossmann sales dataset has {sales_rows} rows and {sales_cols} columns.\")\n",
        "print(f\"Stores dataset has {stores_rows} rows and {stores_cols} columns.\")\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Rossmann Sales Dataset Info\n",
        "rossmann_sales_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stores dataset info\n",
        "stores_df.info()"
      ],
      "metadata": {
        "id": "Fm7nR3QUFs4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "# Count duplicate values in rossmann_sales_df\n",
        "duplicate_count_sales = rossmann_sales_df.duplicated().sum()\n",
        "\n",
        "# Count duplicate values in stores_df\n",
        "duplicate_count_stores = stores_df.duplicated().sum()\n",
        "\n",
        "print(f\"Duplicate count in Rossmann Sales DataFrame: {duplicate_count_sales}\")\n",
        "print(f\"Duplicate count in Stores DataFrame: {duplicate_count_stores}\")\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "# Count missing values in rossmann_sales_df\n",
        "missing_count_sales = rossmann_sales_df.isnull().sum()\n",
        "\n",
        "# Count missing values in stores_df\n",
        "missing_count_stores = stores_df.isnull().sum()\n",
        "\n",
        "print(\"Null Value Counts in Rossmann Sales DataFrame:\")\n",
        "print(\"-\"*45)\n",
        "print(missing_count_sales)\n",
        "\n",
        "print(\"\\nNull Value Counts in Stores DataFrame:\")\n",
        "print(\"-\"*38)\n",
        "print(missing_count_stores)\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizing the missing values\n",
        "# Calculate the percentage of missing values in each column for stores_df\n",
        "missing_percent_stores = (stores_df.isnull().sum() / len(stores_df)) * 100\n",
        "\n",
        "# Create a bar plot to visualize the missing values\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=missing_percent_stores.index, y=missing_percent_stores.values, palette='coolwarm')\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Percentage of Missing Values')\n",
        "plt.title('Percentage of Missing Values in Stores DataFrame')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ENqqxHKig9v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights Gain-**\n",
        "\n",
        "CompetitionDistance: There are only 3 missing values for the CompetitionDistance column. This indicates that most stores have their competition distances recorded.\n",
        "\n",
        "CompetitionOpenSinceMonth and CompetitionOpenSinceYear: These two columns have 354 missing values each, which means a significant number of stores do not have information about their competition's opening month and year.\n",
        "\n",
        "Promo2SinceWeek and Promo2SinceYear: Similar to CompetitionOpenSinceMonth and CompetitionOpenSinceYear, these two columns also have 544 missing values each. This suggests that a substantial number of stores lack data on when they started participating in Promo2 (a continuing and consecutive promotion).\n",
        "\n",
        "PromoInterval: The PromoInterval column also has 544 missing values, indicating that many stores do not have information about the intervals at which they participate in Promo2."
      ],
      "metadata": {
        "id": "mu5MBxbEjHFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Now We try to handle missing values in stores dataset:-`**"
      ],
      "metadata": {
        "id": "gM9-JpnmvGga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of 1115 entries there are missing values for the columns:\n",
        "* CompetitionDistance- distance in meters to the nearest competitor store, the distribution plot would give us an idea about the distances at which generally the stores are opened and we would impute the values accordingly.\n",
        "\n",
        "* CompetitionOpenSinceMonth- gives the approximate month of the time the nearest competitor was opened, mode of the column would tell us the most occuring month    \n",
        "* CompetitionOpenSinceYear-  gives the approximate year of the time the nearest competitor was opened, mode of the column would tell us the most occuring month    \n",
        "* Promo2SinceWeek, Promo2SinceYear and PromoInterval are NaN wherever Promo2 is 0 or False as can be seen in the first look of the dataset. They can be replaced with 0."
      ],
      "metadata": {
        "id": "wWPz8YmmxAYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution plot of competition distance\n",
        "sns.distplot(x=stores_df['CompetitionDistance'], hist = True)\n",
        "plt.xlabel('Competition Distance Distribution Plot')"
      ],
      "metadata": {
        "id": "fwvWas4zxXbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like most of the values of the CompetitionDistance are towards the left and the distribution is skewed on the right. Median is more robust to outlier effect."
      ],
      "metadata": {
        "id": "Zix0ea8uxovG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filling competition distance with the median value\n",
        "stores_df['CompetitionDistance'].fillna(stores_df['CompetitionDistance'].median(), inplace = True)\n",
        "stores_df['CompetitionDistance'] = stores_df['CompetitionDistance'].astype('int64')\n"
      ],
      "metadata": {
        "id": "pwiv0MXsxsMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filling competition open since month and year with the most occuring values of the columns i.e modes of those columns\n",
        "stores_df['CompetitionOpenSinceMonth'].fillna(stores_df['CompetitionOpenSinceMonth'].mode()[0], inplace = True)\n",
        "stores_df['CompetitionOpenSinceYear'].fillna(stores_df['CompetitionOpenSinceYear'].mode()[0], inplace = True)"
      ],
      "metadata": {
        "id": "nLjq2xASyELh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputing the nan values of promo2 related columns with 0\n",
        "stores_df['Promo2SinceWeek'].fillna(value=0,inplace=True)\n",
        "stores_df['Promo2SinceYear'].fillna(value=0,inplace=True)\n",
        "stores_df['PromoInterval'].fillna(value=0,inplace=True)"
      ],
      "metadata": {
        "id": "psR5BNKjyRMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check again for missing value if anyone left\n",
        "stores_df.isnull().sum()"
      ],
      "metadata": {
        "id": "np05uxDoyltJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rossmann Stores Data.csv - historical data including Sales\n",
        "#### store.csv  - supplemental information about the stores\n",
        "\n",
        "\n",
        "#### <u>Data fields</u>\n",
        "#### Most of the fields are self-explanatory.\n",
        "\n",
        "\n",
        "\n",
        "* **Id** - an Id that represents a (Store, Date) duple within the set\n",
        "*  **Store** - a unique Id for each store\n",
        "*  **Sales** - the turnover for any given day (Dependent Variable)\n",
        "* **Customers** - the number of customers on a given day\n",
        "* **Open** - an indicator for whether the store was open: 0 = closed, 1 = open\n",
        "* **StateHoliday** - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
        "* **SchoolHoliday** - indicates if the (Store, Date) was affected by the closure of public schools\n",
        "* **StoreType** - differentiates between 4 different store models: a, b, c, d\n",
        "* **Assortment** - describes an assortment level: a = basic, b = extra, c = extended. An assortment strategy in retailing involves the number and type of products that stores display for purchase by consumers.\n",
        "* **CompetitionDistance** - distance in meters to the nearest competitor store\n",
        "* **CompetitionOpenSince**[Month/Year] - gives the approximate year and month of the time the nearest competitor was opened\n",
        "* **Promo** - indicates whether a store is running a promo on that day\n",
        "* **Promo2** - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n",
        "* **Promo2Since**[Year/Week] - describes the year and calendar week when the store started participating in Promo2\n",
        "* **PromoInterval** - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "# Explore each column in rossmann_sales_df\n",
        "for column in rossmann_sales_df.columns:\n",
        "    print(f\"Column: {column}\")\n",
        "    print(\"Data Type:\", rossmann_sales_df[column].dtype)\n",
        "    print(\"Number of Unique Values:\", rossmann_sales_df[column].nunique())\n",
        "    print(\"Value Counts:\")\n",
        "    print(rossmann_sales_df[column].value_counts())\n",
        "    print(\"-\" * 30)\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From the output, we can gain the following insights about the \"rossmann_sales_df\" dataset** :-\n",
        "\n",
        "1. Store: There are 1115 unique stores in the dataset, and most store appears 942 times and few appears 758 times except for one store that appears only 1 time i.e index 988.\n",
        "\n",
        "2. DayOfWeek: There are 7 unique days of the week, and each day appears roughly the same number of times, with Sunday and Monday having slightly fewer occurrences compared to the other days.\n",
        "\n",
        "3. Date: The dataset covers a period of time from 2013-01-01 to 2015-07-31. Some dates (e.g., 2015-07-31) appear 1115 times, while others appear 935 times.\n",
        "\n",
        "4. Sales: There are 21,734 unique sales values. Most of the sales are non-zero, with 0 appearing 172,871 times.\n",
        "\n",
        "5. Customers: There are 4,086 unique customer counts. Most of the time, the number of customers is non-zero, with 0 appearing 172,869 times.\n",
        "\n",
        "6. Open: There are two unique values for the \"Open\" column (0 and 1). Most of the time, the store is open (1) with 844,392 occurrences, and closed (0) with 172,817 occurrences.\n",
        "\n",
        "7. Promo: There are two unique values for the \"Promo\" column (0 and 1). There are more days without promotions (0) than with promotions (1).\n",
        "\n",
        "8. StateHoliday: There are four unique values in the \"StateHoliday\" column, including '0', 'a', 'b', and 'c'. Most of the time, it is not a state holiday ('0'), with 986,159 occurrences.\n",
        "\n",
        "9. SchoolHoliday: There are two unique values for the \"SchoolHoliday\" column (0 and 1). There are more days without school holidays (0) than with school holidays (1)."
      ],
      "metadata": {
        "id": "p5R9_7vkpa7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets explore few of these columns that have irregularities in rossmann sales df."
      ],
      "metadata": {
        "id": "Lr9MzAPjdfNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Date`- Change datatype from object to datetime and extract year, month and days of month."
      ],
      "metadata": {
        "id": "giwKdVRkdu9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert \"Date\" column to datetime datatype\n",
        "rossmann_sales_df['Date'] = pd.to_datetime(rossmann_sales_df['Date'])\n",
        "\n",
        "# Extract year, month, and day of the week\n",
        "rossmann_sales_df['Year'] = rossmann_sales_df['Date'].dt.year\n",
        "rossmann_sales_df['Month'] = rossmann_sales_df['Date'].dt.month\n",
        "rossmann_sales_df['DayOfMonth'] = rossmann_sales_df['Date'].dt.day\n",
        "\n",
        "print(\"Updated Rossman Sales DataFrame:\")\n",
        "rossmann_sales_df.head()\n"
      ],
      "metadata": {
        "id": "yDp-wZWLeECS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Sales` & `Customers`- identify those rows that zero sale value or zero customers."
      ],
      "metadata": {
        "id": "vzaVQxnch0Wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how many rows have 0 sales value\n",
        "num_rows_with_zero_sales = len(rossmann_sales_df[rossmann_sales_df['Sales'] == 0])\n",
        "print(\"Number of rows with 0 sales value:\", num_rows_with_zero_sales)\n",
        "\n",
        "# Count how many rows have 0 customer value\n",
        "num_rows_with_zero_customers = len(rossmann_sales_df[rossmann_sales_df['Customers'] == 0])\n",
        "print(\"Number of rows with 0 customer value:\", num_rows_with_zero_customers)\n",
        "# Drop rows where Sales is equal to 0\n",
        "#rossmann_sales_df = rossmann_sales_df.loc[rossmann_sales_df['Sales'] != 0]\n",
        "# Drop rows where Customers is equal to 0\n",
        "#rossmann_sales_df = rossmann_sales_df.loc[rossmann_sales_df['Customers'] != 0]"
      ],
      "metadata": {
        "id": "JsS_8MYuiv5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`StateHoliday`- The feature StateHoliday changed into a boolean variable. The value {a, b, c} became 1, other 0;"
      ],
      "metadata": {
        "id": "zmPCrnWuvbI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rossmann_sales_df['StateHoliday'] = rossmann_sales_df['StateHoliday'].replace({'a': 1, 'b': 1, 'c': 1, '0': 0})"
      ],
      "metadata": {
        "id": "kdg1Jl62uQVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reason for doing this**-\n",
        "\n",
        "we are replacing the values in the StateHoliday column with numerical representations. The reason to do this is to convert the categorical values ('a', 'b', 'c', and '0') into numerical equivalents (1 and 0) so that they can be used in numerical computations and analysis.\n",
        "\n",
        "Machine learning models typically require numerical data to perform calculations and make predictions. By converting categorical variables to numerical representations, you can include them in your analysis and model training."
      ],
      "metadata": {
        "id": "T3MpP2EIv_E1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "# Explore each column in stores_df\n",
        "for column in stores_df.columns:\n",
        "    print(f\"Column: {column}\")\n",
        "    print(\"Data Type:\", stores_df[column].dtype)\n",
        "    print(\"Number of Unique Values:\", stores_df[column].nunique())\n",
        "    print(\"Value Counts:\")\n",
        "    print(stores_df[column].value_counts())\n",
        "    print(\"-\" * 30)\n"
      ],
      "metadata": {
        "id": "yMyersO-xkH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**From the exploration of the stores_df, we can gain the following insights:-**\n",
        "\n",
        "1. Store column has 1115 unique store IDs, and each store appears only once in the dataset.\n",
        "\n",
        "2. StoreType column has 4 unique store types, with store type \"a\" being the most common (602 occurrences).\n",
        "\n",
        "3. Assortment column has 3 unique assortments, with assortment type \"a\" being the most common (593 occurrences).\n",
        "\n",
        "4. CompetitionDistance column is a numerical feature representing the distance to the nearest competitor. It has 655 unique values, and the most common distance is 250.0 (12 occurrences).\n",
        "\n",
        "5. CompetitionOpenSinceMonth column represents the month when the nearest competitor opened. It has 12 unique values, and the most common value is 9.0 (479 occurrences).\n",
        "\n",
        "6. CompetitionOpenSinceYear column represents the year when the nearest competitor opened. It has 23 unique values, and the most common value is 2013.0 (437 occurrences).\n",
        "\n",
        "7. Promo2 column is a binary feature indicating whether the store is participating in Promo2 or not. There are 571 stores with Promo2 and 544 stores without Promo2.\n",
        "8. Promo2SinceWeek column represents the calendar week when Promo2 started for each store. It has 25 unique values, and 544 stores have no Promo2 (value 0.0).\n",
        "\n",
        "9. Promo2SinceYear column represents the year when Promo2 started for each store. It has 8 unique values, and 544 stores have no Promo2 (value 0.0).\n",
        "\n",
        "10. PromoInterval column represents the consecutive intervals Promo2 is started, indicating which months the promotion is active. It has 4 unique values, with \"Jan,Apr,Jul,Oct\" being the most common (335 occurrences)."
      ],
      "metadata": {
        "id": "zVC_x47UyiZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Type Conversion**: The columns `CompetitionOpenSinceMonth`, `CompetitionOpenSinceYear`, `Promo2SinceWeek`, and `Promo2SinceYear` are represented as floating-point numbers. These columns should be converted to integers since they represent months and years, which are whole numbers.\n",
        "\n"
      ],
      "metadata": {
        "id": "6yjadsEy0mBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data type conversion\n",
        "stores_df['CompetitionOpenSinceMonth'] = stores_df['CompetitionOpenSinceMonth'].astype('Int64')\n",
        "stores_df['CompetitionOpenSinceYear'] = stores_df['CompetitionOpenSinceYear'].astype('Int64')\n",
        "stores_df['Promo2SinceWeek'] = stores_df['Promo2SinceWeek'].astype('Int64')\n",
        "stores_df['Promo2SinceYear'] = stores_df['Promo2SinceYear'].astype('Int64')\n"
      ],
      "metadata": {
        "id": "yHoLTZQu1mTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will do categorical encoding for `StoreType`, `Assortment`, and `PromoInterval` in Feature Engineering ."
      ],
      "metadata": {
        "id": "gYsY-z_M41EZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since most of the stores open between 1990 to 2015, there are two store that opens in 1900 and 1961 so we drop them as think of them as outliers or typing mistake while creating dataset."
      ],
      "metadata": {
        "id": "NLxAmbV27Oa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with CompetitionOpenSinceYear values 1900 and 1961\n",
        "stores_df = stores_df[~stores_df['CompetitionOpenSinceYear'].isin([1900, 1961])]\n",
        "\n",
        "# Reset the index after dropping rows\n",
        "stores_df.reset_index(drop=True, inplace=True)\n"
      ],
      "metadata": {
        "id": "9iFD9J4P7p9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "# Describe rossmann_sales_df\n",
        "rossmann_sales_df_description = rossmann_sales_df.describe()\n",
        "print(\"Description of Rossmann Sales DataFrame:\")\n",
        "rossmann_sales_df_description.T\n",
        "\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Describe stores_df\n",
        "stores_df_description = stores_df.describe()\n",
        "print(\"\\nDescription of Stores DataFrame:\")\n",
        "stores_df_description.T\n"
      ],
      "metadata": {
        "id": "8VBM5dZ2-pCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights from Rossmann Sales DataFrame Description**:\n",
        "\n",
        "1. The Sales column ranges from 46 to 41,551 with a mean of approximately 6,956. This indicates that there is a wide variation in sales across different stores and days.\n",
        "2. The Customers column ranges from 8 to 7,388 with a mean of approximately 763. This suggests that there is variation in the number of customers visiting the stores.\n",
        "3. The Open column that has a single value of 1, indicating that all stores are open otherwise digit 0 indicating store are closed in the dataset.\n",
        "4. The Promo column has a mean of approximately 0.45, indicating that promotions are active in around 45% of the cases.\n",
        "5. The StateHoliday column is mostly filled with the value 0, with very few occurrences of 'a', 'b', and 'c'.\n",
        "6. The SchoolHoliday column indicates that around 19% of the data corresponds to school holidays.\n",
        "\n",
        "**Insights from Stores DataFrame Description**:\n",
        "\n",
        "1. The CompetitionDistance column varies widely, ranging from 20 to 75,860. This indicates that the distance to the nearest competitor can significantly differ between stores.\n",
        "\n",
        "2. The CompetitionOpenSinceMonth and CompetitionOpenSinceYear columns show the month and year of when the competition opened, respectively. Most stores have values in the month of 9 and the year 2013.\n",
        "\n",
        "3. The Promo2 column has values 0 and 1, indicating whether the store is participating in Promo2 or not.\n",
        "4. The Promo2SinceWeek and Promo2SinceYear columns represent the week and year when Promo2 started for each store.\n",
        "\n",
        "5. The columns StoreType and Assortment are categorical, and they have multiple unique values indicating different store types and assortments."
      ],
      "metadata": {
        "id": "G7IG5GqdH9_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variable Description for Rossmann Sales DataFrame (rossmann_sales_df):\n",
        "\n",
        "Store: The store ID.\n",
        "\n",
        "DayOfWeek: The day of the week (1: Monday, 2: Tuesday, ..., 7: Sunday).\n",
        "\n",
        "Date: The date of the sales record.\n",
        "\n",
        "Sales: The sales value for that particular day and store.\n",
        "\n",
        "Customers: The number of customers on that day and store.\n",
        "\n",
        "Open: Indicates whether the store was open on that day (1: Open, 0: Closed).\n",
        "\n",
        "Promo: Indicates whether a promotional offer was active on that day (1: Yes, 0: No).\n",
        "\n",
        "StateHoliday: Indicates whether it is a state holiday (0: Not a holiday, 1: Public holiday, 2: Easter holiday, 3: Christmas holiday).\n",
        "\n",
        "SchoolHoliday: Indicates whether it is a school holiday (1: Yes, 0: No).\n",
        "\n",
        "Year: The year extracted from the Date column.\n",
        "\n",
        "Month: The month extracted from the Date column.\n",
        "\n",
        "DayOfMonth: The day of the month extracted from the Date column.\n",
        "\n",
        "Variable Description for Stores DataFrame (stores_df):\n",
        "\n",
        "Store: The store ID.\n",
        "\n",
        "StoreType: The type of store (a, b, c, d).\n",
        "\n",
        "Assortment: The assortment type (a: Basic, b: Extra, c: Extended).\n",
        "\n",
        "CompetitionDistance: The distance to the nearest competitor store.\n",
        "\n",
        "CompetitionOpenSinceMonth: The month when the nearest competitor store opened.\n",
        "\n",
        "CompetitionOpenSinceYear: The year when the nearest competitor store opened.\n",
        "\n",
        "Promo2: Indicates whether the store is participating in Promo2 (1: Yes, 0: No).\n",
        "\n",
        "Promo2SinceWeek: The week when Promo2 started for the store.\n",
        "\n",
        "Promo2SinceYear: The year when Promo2 started for the store.\n",
        "\n",
        "PromoInterval: The interval of Promo2 for the store (e.g., Jan, Apr, Jul, Oct)."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "# Unique Values for each variable in rossmann_sales_df (excluding 'Date')\n",
        "for column in rossmann_sales_df.columns:\n",
        "    if column != 'Date':\n",
        "        unique_values = rossmann_sales_df[column].unique()\n",
        "        print(f\"Unique Values for {column}:\\n{unique_values}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initially we have two values of `Open` column i.e 0 and 1 but when we drop those rows that have zero sales and zero customers then automatically it also drop those value of 0 in `Open` column too so in the end we have only single digit 1 that stands for stores are open."
      ],
      "metadata": {
        "id": "X-Ib2hxEMKzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique Values for each variable in stores_df\n",
        "for column in stores_df.columns:\n",
        "    unique_values = stores_df[column].unique()\n",
        "    print(f\"Unique Values for {column}:\\n{unique_values}\\n\")\n"
      ],
      "metadata": {
        "id": "7blBmla-qAbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Merge the datasets on the 'Store' column\n",
        "merged_df = pd.merge(rossmann_sales_df, stores_df, on='Store', how='inner')\n",
        "\n",
        "# Display the merged DataFrame\n",
        "merged_df\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***UNIVARIATE ANALYSIS***"
      ],
      "metadata": {
        "id": "iDc6HHFMKrNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question-1: Distribution of Sales?\n",
        "# Summary Statistics for Sales\n",
        "sales_summary = merged_df['Sales'].describe()\n",
        "print(\"Summary Statistics for Sales:\")\n",
        "print(sales_summary)\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "Y-_glMUmMXsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question -2 : Distribution among some important features?\n",
        "\n",
        "# Select columns for summary statistics\n",
        "selected_columns = ['Promo', 'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment', 'PromoInterval']\n",
        "\n",
        "# Create an empty DataFrame to store the summary statistics\n",
        "summary_df = pd.DataFrame()\n",
        "\n",
        "# Calculate summary statistics for each column and append to the summary_df\n",
        "for column in selected_columns:\n",
        "    summary = merged_df[column].describe().to_frame(column)\n",
        "    summary_df = pd.concat([summary_df, summary], axis=1)\n",
        "\n",
        "print(\"Summary Statistics for Categorical Variables:\")\n",
        "summary_df\n"
      ],
      "metadata": {
        "id": "eaPE9BPdYifM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question - 3: calculate and visualize the average number of visits per customer, the average spending per customer, and the customer retention rate.\n",
        "\n",
        "# Calculate average number of visits per customer\n",
        "visits_per_customer = merged_df.groupby('Customers')['Date'].count().mean()\n",
        "print(f\"visits per customer:{visits_per_customer}\")\n",
        "# Calculate average spending per customer\n",
        "average_spending_per_customer = merged_df.groupby('Customers')['Sales'].mean().mean()\n",
        "print(f\"average spending per customer:{average_spending_per_customer}\")\n",
        "# Calculate customer retention rate\n",
        "unique_customers_per_month = merged_df.groupby(['Year', 'Month'])['Customers'].nunique()\n",
        "total_customers_per_month = merged_df.groupby(['Year', 'Month'])['Customers'].count()\n",
        "customer_retention_rate = (unique_customers_per_month / total_customers_per_month).mean()\n",
        "\n",
        "\n",
        "# Customer Retention Rate\n",
        "print(f\"Customer Retention Rate: {customer_retention_rate:.2%}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "H7TbRpknm2eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 4: Calculate the average number of promotional days per week or month and analyze its impact on sales.\n",
        "# Calculate average sales for days with promotions (Promo = 1)\n",
        "promo_days_sales = merged_df[merged_df['Promo'] == 1]['Sales'].mean()\n",
        "\n",
        "# Calculate average sales for days without promotions (Promo = 0)\n",
        "non_promo_days_sales = merged_df[merged_df['Promo'] == 0]['Sales'].mean()\n",
        "\n",
        "# Calculate the promotional intensity\n",
        "promotional_intensity = promo_days_sales / non_promo_days_sales\n",
        "\n",
        "print(f\"Promotional Intensity: {promotional_intensity:.2f}\")\n"
      ],
      "metadata": {
        "id": "XS6mdtSltpGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***BIVARIATE ANALYSIS***"
      ],
      "metadata": {
        "id": "02CXGwxuc-oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question- 5: Effects of customers, promo and dayofweek on sales?\n",
        "\n",
        "# Calculate mean sales for each unique combination of customers, promo, and day of week\n",
        "grouped_df = merged_df.groupby(['Customers', 'Promo', 'DayOfWeek'])['Sales'].mean().reset_index()\n",
        "\n",
        "# Create a new DataFrame with columns for 'Customers', 'Promo', 'DayOfWeek', and 'SalesMean'\n",
        "new_df = pd.DataFrame(columns=['Customers', 'Promo', 'DayOfWeek', 'SalesMean'])\n",
        "\n",
        "# Populate the new DataFrame\n",
        "for _, group in grouped_df.groupby(['Customers', 'Promo']):\n",
        "    customers, promo = _\n",
        "    group_sorted = group.sort_values('DayOfWeek')\n",
        "    sales_mean_values = group_sorted['Sales'].values\n",
        "    is_increasing = np.all(np.diff(sales_mean_values) >= 0)\n",
        "    new_df = new_df.append({\n",
        "        'Customers': customers,\n",
        "        'Promo': promo,\n",
        "        'DayOfWeek': group_sorted['DayOfWeek'].values,\n",
        "        'SalesMean': sales_mean_values,\n",
        "        'IsIncreasing': is_increasing\n",
        "    }, ignore_index=True)\n",
        "\n",
        "new_df\n"
      ],
      "metadata": {
        "id": "c7nIwXvgdFjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question- 6: Effects of SchoolHoliday, StateHoliay, StoreType and Assortment on Sales?\n",
        "# Grouping by StateHoliday and calculating mean sales\n",
        "stateholiday_sales = merged_df.groupby('StateHoliday')['Sales'].mean().reset_index()\n",
        "\n",
        "# Grouping by SchoolHoliday and calculating mean sales\n",
        "schoolholiday_sales = merged_df.groupby('SchoolHoliday')['Sales'].mean().reset_index()\n",
        "\n",
        "# Grouping by StoreType and calculating mean sales\n",
        "storetype_sales = merged_df.groupby('StoreType')['Sales'].mean().reset_index()\n",
        "\n",
        "# Grouping by Assortment and calculating mean sales\n",
        "assortment_sales = merged_df.groupby('Assortment')['Sales'].mean().reset_index()\n",
        "\n",
        "# Creating a new DataFrame to store the results\n",
        "sales_by_feature_df = pd.DataFrame({\n",
        "    'StateHoliday': stateholiday_sales['StateHoliday'],\n",
        "    'StateHoliday_SalesMean': stateholiday_sales['Sales'],\n",
        "    'SchoolHoliday': schoolholiday_sales['SchoolHoliday'],\n",
        "    'SchoolHoliday_SalesMean': schoolholiday_sales['Sales'],\n",
        "    'StoreType': storetype_sales['StoreType'],\n",
        "    'StoreType_SalesMean': storetype_sales['Sales'],\n",
        "    'Assortment': assortment_sales['Assortment'],\n",
        "    'Assortment_SalesMean': assortment_sales['Sales']\n",
        "})\n",
        "\n",
        "# Display the new DataFrame\n",
        "sales_by_feature_df\n"
      ],
      "metadata": {
        "id": "WRR9-Um7lHwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question-7 : what is average number of customers per day of week?\n",
        "\n",
        "average_customers_per_day_of_week = merged_df.groupby('DayOfWeek')['Customers'].mean()\n",
        "\n",
        "print(average_customers_per_day_of_week)\n",
        "\n"
      ],
      "metadata": {
        "id": "GLwkM_TXsF_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question- 8: average number of customers on stateholiday and schoolholiday?\n",
        "# Calculate average number of customers on state holidays and non-state holidays\n",
        "state_holiday_customers = merged_df.groupby('StateHoliday')['Customers'].mean()\n",
        "non_state_holiday_customers = merged_df[merged_df['StateHoliday'] == 0]['Customers'].mean()\n",
        "\n",
        "# Calculate average number of customers on school holidays and non-school holidays\n",
        "school_holiday_customers = merged_df.groupby('SchoolHoliday')['Customers'].mean()\n",
        "non_school_holiday_customers = merged_df[merged_df['SchoolHoliday'] == 0]['Customers'].mean()\n",
        "\n",
        "print(\"Average number of customers on state holidays:\", state_holiday_customers[1.0])\n",
        "print(\"Average number of customers on non-state holidays:\", non_state_holiday_customers)\n",
        "print(\"Average number of customers on school holidays:\", school_holiday_customers[1])\n",
        "print(\"Average number of customers on non-school holidays:\", non_school_holiday_customers)\n"
      ],
      "metadata": {
        "id": "o4bSud2B6Ta_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question- 9: what is total number of customers in each storetype and share of each storetype in total stores type?\n",
        "# Calculate the total number of customers in the dataset\n",
        "total_customers =  merged_df['Customers'].sum()\n",
        "\n",
        "# Group the DataFrame by 'StoreType' and calculate the sum of 'Customers' for each store type\n",
        "customer_by_store_type = merged_df.groupby('StoreType')['Customers'].sum()\n",
        "\n",
        "# Calculate the percentage of customers in each store type\n",
        "percentage_customers_by_store_type = (customer_by_store_type / total_customers) * 100\n",
        "\n",
        "print(percentage_customers_by_store_type)\n",
        "# Count the number of stores in each store type\n",
        "store_type_counts = merged_df[\"StoreType\"].value_counts()\n",
        "\n",
        "# Calculate the share of each store type\n",
        "store_type_share = store_type_counts / len(merged_df) * 100\n",
        "print(store_type_share)"
      ],
      "metadata": {
        "id": "drgXSvp9BrTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.dtypes"
      ],
      "metadata": {
        "id": "YdZBhWIHhsoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question- 10: Correlation between sales and competitionDistance.\n",
        "#Data Wrangling for Sales vs. Competition Distance\n",
        "sales_competition_data = merged_df[['Sales', 'CompetitionDistance']]\n",
        "\n",
        "print(sales_competition_data)\n",
        "\n",
        "# Calculate the correlation coefficient between Sales and CompetitionDistance\n",
        "correlation_coefficient = sales_competition_data['Sales'].corr(sales_competition_data['CompetitionDistance'])\n",
        "print(\"correlation_coefficient is:\",correlation_coefficient)\n"
      ],
      "metadata": {
        "id": "mBWjKJAaoBHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question- 11: how sales are varying per day in a month and varying per month in a year?\n",
        "\n",
        "# Group the data by Day of Month and calculate the mean sales for each day\n",
        "sales_by_day_of_month = merged_df.groupby('DayOfMonth')['Sales'].mean().reset_index()\n",
        "print(sales_by_day_of_month)\n",
        "# Group the data by Month and calculate the mean sales for each month\n",
        "sales_by_month = merged_df.groupby('Month')['Sales'].mean().reset_index()\n",
        "print(sales_by_month)"
      ],
      "metadata": {
        "id": "ZPMva8pmsg_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question- 12 : Average Sales Per Year?\n",
        "\n",
        "# Group the data by Year and calculate the mean sales for each year\n",
        "sales_by_year = merged_df.groupby('Year')['Sales'].mean().reset_index()\n",
        "sales_by_year"
      ],
      "metadata": {
        "id": "xKMnMTAL34mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***MULTIVARIATE ANALYSIS***"
      ],
      "metadata": {
        "id": "sjLSPLJRm81j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First,we use a pairplot as It is a visualization technique that allows us to plot pairwise relationships between multiple variables in a dataset.\n",
        "\n",
        "Pairplot is useful for identifying correlations and patterns between multiple variables in a dataset. It can be especially useful in exploratory data analysis, as it allows us to quickly examine the relationships between multiple variables and identify any interesting features or outliers."
      ],
      "metadata": {
        "id": "OTMu4mIDnJfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a pairplot to check correlation between all important features like Sales, Customers, Promo, CompetitionDistance etc.\n",
        "Sales = merged_df['Sales']\n",
        "Customer = merged_df['Customers']\n",
        "Promo = merged_df['Promo']\n",
        "CompetitionDistance = merged_df['CompetitionDistance']\n",
        "Month = merged_df['Month']\n",
        "Year = merged_df['Year']"
      ],
      "metadata": {
        "id": "XB62b-aTnCNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Heatmap for respected feaures to find correlation between them.\n",
        "#we need only meaningful numeric columns here, let's drop the unnecessary to get a clear picture\n",
        "columns_to_drop = ['Store', 'Year', 'Month', 'DayOfMonth']\n",
        "corr_df = merged_df.drop(columns = columns_to_drop, axis =1)"
      ],
      "metadata": {
        "id": "ml_oUuuVsFXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "#Question-1: Distribution of Sales?\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(merged_df['Sales'], kde=True, color='blue')\n",
        "plt.title('Sales Distribution')\n",
        "plt.xlabel('Sales')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked the histogram with a kernel density estimate (KDE) plot for the 'Sales' distribution because it is a suitable choice for visualizing the distribution of a continuous numerical variable like sales. Here are some reasons why the histogram with KDE plot is an appropriate choice:\n",
        "\n",
        "1. Data Distribution: The histogram shows the distribution of sales values across different bins, allowing you to see the frequency of sales values in specific ranges. The KDE plot provides a smooth estimate of the probability density function, giving a clearer view of the underlying data distribution.\n",
        "\n",
        "2. Continuous Data: The 'Sales' column contains continuous numerical data, and a histogram is well-suited to represent this type of data. It can show the range and frequency of sales values, providing an overview of how sales are distributed.\n",
        "\n",
        "3. Skewness and Outliers: The histogram can reveal if the 'Sales' distribution is skewed (asymmetric) or contains outliers. Skewness and outliers can be important insights to understand the sales behavior in the dataset.\n",
        "\n",
        "4. Central Tendency: The histogram allows you to observe the central tendency of the 'Sales' data, which can be further verified through the summary statistics like mean and median."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Distribution Shape: The histogram shows that the 'Sales' distribution is positively skewed, as it extends more towards higher sales values. This suggests that there are relatively more instances of lower sales values and fewer instances of very high sales values.\n",
        "\n",
        "2. Central Tendency: The peak of the histogram (highest frequency bin) indicates the central tendency of the data, which is around 6,000 to 8,000 sales. This is where the most frequent sales values are concentrated.\n",
        "\n",
        "3. Spread and Variability: The spread of the data can be observed by looking at the width of the distribution. The wider the spread, the more variability there is in the sales values. In this case, the spread appears to be moderate, as the distribution covers a range of sales values from around 0 to 40,000.\n",
        "\n",
        "4. Outliers: There are some sales values that are far from the central region, indicated by the long tail to the right. These points represent potential outliers, which are sales values significantly higher than the majority of the data points."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact**-\n",
        "\n",
        "1. Understanding Sales Distribution: The histogram with KDE plot provides an understanding of how sales are distributed across different ranges. This insight can help businesses identify their most common sales figures and assess the overall spread of sales values. By understanding the distribution, businesses can set realistic sales targets, plan inventory management, and allocate resources effectively.\n",
        "\n",
        "2. Identifying Sales Patterns: The shape of the distribution can reveal sales patterns and trends. For example, if the distribution is skewed towards higher sales values, it might indicate that certain products or promotions are particularly popular. This insight can guide marketing strategies to capitalize on successful products and promotions.\n",
        "\n",
        "3. Detecting Outliers: The histogram can highlight potential outliers—sales values that are significantly higher or lower than the majority. Outliers may represent exceptional events or errors in data entry. Identifying and investigating outliers can help businesses pinpoint anomalies and address any issues affecting sales performance.\n",
        "\n",
        "**Negative Business Impact**:-\n",
        "\n",
        "1. Skewed Distribution: If the sales distribution is heavily skewed, with a majority of sales concentrated towards the lower end, it could indicate difficulty in achieving significant sales growth or challenges in expanding market share. In such cases, businesses may need to explore strategies to stimulate demand and attract more customers.\n",
        "\n",
        "2. Sales Decline in Specific Stores: If the analysis reveals a consistent decline in sales in specific stores or regions, it may suggest operational issues or poor market conditions in those areas. Addressing these issues is crucial to avoid further negative impact and focus on areas with higher growth potential.\n",
        "\n",
        "3. No Growth in High-Performing Stores: If the sales distribution shows no significant growth in high-performing stores, it might indicate that the stores have reached their saturation point. In such cases, businesses may need to diversify their offerings, introduce new products, or explore expansion opportunities in other areas."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "#Question -2 : Distribution among some important features?\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Univariate Analysis - Promo Distribution\n",
        "sns.countplot(data=merged_df, x='Promo', palette='pastel', ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Promo Distribution')\n",
        "axes[0, 0].set_xlabel('Promo')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "\n",
        "# Univariate Analysis - StateHoliday Distribution\n",
        "sns.countplot(data=merged_df, x='StateHoliday', palette='Set2', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('StateHoliday Distribution')\n",
        "axes[0, 1].set_xlabel('StateHoliday')\n",
        "axes[0, 1].set_ylabel('Count')\n",
        "\n",
        "# Univariate Analysis - SchoolHoliday Distribution\n",
        "sns.countplot(data=merged_df, x='SchoolHoliday', palette='Set3', ax=axes[0, 2])\n",
        "axes[0, 2].set_title('SchoolHoliday Distribution')\n",
        "axes[0, 2].set_xlabel('SchoolHoliday')\n",
        "axes[0, 2].set_ylabel('Count')\n",
        "\n",
        "# Univariate Analysis - StoreType Distribution\n",
        "sns.countplot(data=merged_df, x='StoreType', palette='tab10', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('StoreType Distribution')\n",
        "axes[1, 0].set_xlabel('StoreType')\n",
        "axes[1, 0].set_ylabel('Count')\n",
        "\n",
        "# Univariate Analysis - Assortment Distribution\n",
        "sns.countplot(data=merged_df, x='Assortment', palette='Set1', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Assortment Distribution')\n",
        "axes[1, 1].set_xlabel('Assortment')\n",
        "axes[1, 1].set_ylabel('Count')\n",
        "\n",
        "# Univariate Analysis - PromoInterval Distribution\n",
        "sns.countplot(data=merged_df, x='PromoInterval', palette='husl', ax=axes[1, 2])\n",
        "axes[1, 2].set_title('PromoInterval Distribution')\n",
        "axes[1, 2].set_xlabel('PromoInterval')\n",
        "axes[1, 2].set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I choose the countplot for the distribution of categorical variables (promo, state holiday, school holiday, store type, assortment, and promo interval) because it is a suitable plot for visualizing the frequency of each category in a categorical variable.\n",
        "\n",
        "The countplot is a bar plot that shows the number of occurrences of each category in a categorical variable. It provides an easy way to compare the frequencies of different categories and quickly identify the most common and least common categories."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight Gain**-\n",
        "\n",
        "1. Promo Distribution:\n",
        "\n",
        "*   The countplot for Promo will show the frequency of days with promotional activities (1) and without promotions (0).\n",
        "*   We can observe the balance between promotional and non-promotional days.\n",
        "\n",
        "*   The countplot will help us identify the proportion of days with promotions compared to days without promotions.\n",
        "\n",
        "2. StateHoliday Distribution:\n",
        "\n",
        "*   The countplot for StateHoliday will show the frequency of different state holidays (0,1).\n",
        "*   We can observe how many days fall under each state holiday category.\n",
        "*   The plot will help us understand the distribution of state holidays in the dataset.\n",
        "\n",
        "3. SchoolHoliday Distribution:\n",
        "\n",
        "* The countplot for SchoolHoliday will show the frequency of school holidays (1) and non-school holidays (0).\n",
        "\n",
        "* We can observe the proportion of days with school holidays compared to regular days.\n",
        "* The plot will help us understand the distribution of school holidays in the dataset.\n",
        "\n",
        "\n",
        "4. StoreType Distribution:\n",
        "\n",
        "* The countplot for StoreType will show the frequency of different store types (a, b, c, d).\n",
        "* We can observe the proportion of each store type in the dataset.\n",
        "* The plot will help us understand the distribution of store types.\n",
        "\n",
        "\n",
        "5. Assortment Distribution:\n",
        "\n",
        "* The countplot for Assortment will show the frequency of different assortments (a, b, c).\n",
        "* We can observe the proportion of each assortment type in the dataset.\n",
        "* The plot will help us understand the distribution of assortments.\n",
        "\n",
        "\n",
        "6. PromoInterval Distribution:\n",
        "\n",
        "* The countplot for PromoInterval will show the frequency of different promo intervals (0, Jan,Apr,Jul,Oct, Feb,May,Aug,Nov, Mar,Jun,Sept,Dec).\n",
        "* We can observe the proportion of each promo interval in the dataset.\n",
        "* The plot will help us understand the distribution of promo intervals."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "* Understanding the distribution of promotional days (Promo) can help the business optimize their promotional strategies. They can focus more on days with high sales potential (promotional days) to boost revenue and customer footfall.\n",
        "* Knowing the distribution of state holidays (StateHoliday) and school holidays (SchoolHoliday) can help in planning marketing campaigns or special offers during those periods to attract more customers and increase sales.\n",
        "* Analyzing the distribution of store types (StoreType) and assortments (Assortment) can help identify which types are performing better in terms of sales and profitability. This insight can guide decisions on store expansion or product assortment optimization.\n",
        "\n",
        "**Insights Leading to Negative Growth:**\n",
        "\n",
        "* While the summary statistics don't explicitly indicate any insights leading to negative growth, it's essential to conduct further analysis and consider the context of the business.\n",
        "* Negative growth may occur if promotional activities (Promo) are not strategically planned and end up being ineffective or costly without generating significant sales.\n",
        "* Poor performance during state holidays (StateHoliday) or school holidays (SchoolHoliday) may indicate missed opportunities or a lack of appealing offers during peak shopping periods, leading to negative growth.\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "#Question - 3: calculate and visualize the average number of visits per customer, the average spending per customer, and the customer retention rate.\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Average Number of Visits per Customer\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(merged_df.groupby('Customers')['Date'].count(), kde=True, color='purple')\n",
        "plt.title('Distribution of Number of Visits per Customer')\n",
        "plt.xlabel('Number of Visits')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Average Spending per Customer\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(merged_df.groupby('Customers')['Sales'].mean(), kde=True, color='orange')\n",
        "plt.title('Distribution of Average Spending per Customer')\n",
        "plt.xlabel('Average Spending')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histograms (histplot in Seaborn) are commonly used to visualize the distribution of a single variable, such as the distribution of sales or customer visits. However, for the specific calculations of visits per customer, average spending per customer, and customer retention rate, we don't need to create histograms."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average Number of Visits per Customer: The average number of visits per customer is approximately 206. This means, on average, each customer visits the store about 206 times during the entire period covered by the dataset. This information can help the business understand customer engagement and frequency of store visits.\n",
        "\n",
        "Average Spending per Customer: The average spending per customer is approximately 13,917. This means, on average, each customer spends around 13,917 units of currency during their visits. This insight can help the business understand customer buying behavior and overall sales potential.\n",
        "\n",
        "Customer Retention Rate: The customer retention rate is approximately 7.72%. This indicates that, on average, only about 7.72% of the customers return to the store in the subsequent months. A low retention rate may suggest that there might be opportunities to improve customer loyalty and engagement strategies."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number of Visits per Customer:**\n",
        "\n",
        "* Positive Impact: A high number of visits per customer indicates that customers are frequently engaging with the business and making repeat visits. This suggests that customers find value in the products or services offered by the business and are loyal to the brand. A high number of visits can lead to increased customer satisfaction, higher customer lifetime value, and positive word-of-mouth, which can attract new customers.\n",
        "\n",
        "* Negative Growth: A low number of visits per customer may indicate that customers are not returning to the business or that they are not satisfied with the products or services. This could lead to lower customer retention, reduced sales, and negative reviews, which can impact the business's reputation and growth negatively.\n",
        "\n",
        "**Average Spending per Customer:**\n",
        "\n",
        "* Positive Impact: A high average spending per customer signifies that customers are willing to spend more money per visit. This can lead to increased revenue and profitability for the business. Customers with high average spending are likely to be more valuable to the business and may also have a higher likelihood of making large purchases in the future.\n",
        "\n",
        "* Negative Growth: A low average spending per customer may suggest that customers are making small purchases or are not willing to spend much on the products or services offered by the business. This could result in lower revenue and reduced profitability.\n",
        "\n",
        "**Customer Retention Rate:**\n",
        "\n",
        "* Positive Impact: A high customer retention rate indicates that the business has been successful in retaining its existing customers over time. High customer retention is crucial for long-term business success as it reduces the need to constantly acquire new customers. Satisfied and loyal customers are more likely to make repeat purchases and refer others, leading to sustainable business growth.\n",
        "\n",
        "* Negative Growth: A low customer retention rate suggests that the business is struggling to retain its customers. This could be due to various factors such as poor customer service, product quality issues, or intense competition. Low customer retention can lead to increased customer acquisition costs and reduced overall revenue."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "#Question 4: Calculate the average number of promotional days per week or month and analyze its impact on sales.\n",
        "\n",
        "# Visualize the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=['Promo Days', 'Non-Promo Days'], y=[promo_days_sales, non_promo_days_sales], palette='pastel')\n",
        "plt.title('Average Sales: Promo Days vs. Non-Promo Days')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar plot is commonly used to display and compare the values of different categories or groups. In the context of promotional intensity analysis, we can use a bar plot to visualize the promotional intensity values for different periods or categories."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output \"Promotional Intensity: 1.81\" represents the average sales on promotional days compared to non-promotional days. The value 1.81 indicates that, on average, the sales on promotional days are 1.81 times higher than the sales on non-promotional days."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Impact: The higher promotional intensity suggests that promotions are effective in boosting sales and attracting customers.\n",
        "\n",
        "Potential for Growth: It indicates the potential for further growth by strategically planning and executing more effective promotions.\n",
        "\n",
        "Seasonal Variation: Seasonal variations in promotional intensity can be leveraged to maximize impact during peak seasons or specific events.\n",
        "\n",
        "Negative Impact-\n",
        "\n",
        " Ineffective Promotions: If certain promotions fail to drive significant sales or customer engagement, it may be necessary to reassess the promotional strategies and make improvements.\n",
        "\n",
        "Customer Churn: If the customer retention rate is low, it could indicate that the business is struggling to retain customers, which may negatively impact revenue and growth."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Bivariate Analysis - Sales vs. Customers\n",
        "sns.scatterplot(data=merged_df, x='Customers', y='Sales', ax=axes[0])\n",
        "axes[0].set_title('Sales vs. Customers')\n",
        "axes[0].set_xlabel('Customers')\n",
        "axes[0].set_ylabel('Sales')\n",
        "\n",
        "# Bivariate Analysis - Sales vs. Promo\n",
        "sns.boxplot(data=merged_df, x='Promo', y='Sales', ax=axes[1])\n",
        "axes[1].set_title('Sales vs. Promo')\n",
        "axes[1].set_xlabel('Promo')\n",
        "axes[1].set_ylabel('Sales')\n",
        "\n",
        "# Bivariate Analysis - Sales vs. Day of Week\n",
        "sns.barplot(data=merged_df, x='DayOfWeek', y='Sales', ax=axes[2])\n",
        "axes[2].set_title('Sales vs. Day of Week')\n",
        "axes[2].set_xlabel('Day of Week')\n",
        "axes[2].set_ylabel('Sales')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sales vs. Customers: I chose a scatter plot because both 'Sales' and 'Customers' are continuous numerical variables. A scatter plot shows the relationship between two continuous variables and helps us observe any patterns or trends in how sales vary with the number of customers.\n",
        "\n",
        "Sales vs. Promo: I chose a box plot because 'Promo' is a categorical variable (0 or 1), and we want to see the distribution of sales for each promo status. A box plot provides us with information about the median, quartiles, and outliers, which can help us understand how sales differ between promo days and non-promo days.\n",
        "\n",
        "Sales vs. Day of Week: I chose a bar plot because 'DayOfWeek' is a categorical variable representing the days of the week (1 to 7). A bar plot allows us to compare the average sales for each day of the week and identify any patterns or differences in sales across different days."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By above charts we can predict that there are increase in sales when number of customers are increasing and their are promotional activity then sales are increasing and in case of dayofweek sales are most often high when there is no sunday.\n",
        "By analyzing this output, we can identify patterns in sales behavior based on the number of customers, promo status, and day of the week. We can observe which combinations lead to increasing sales and which combinations may require further investigation or improvement strategies. The 'IsIncreasing' column provides a quick summary of whether the sales trend is positive or not for each combination."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "* By analyzing the 'SalesMean' column, we can identify the combinations of 'Customers', 'Promo', and 'DayOfWeek' that lead to higher average sales. This information can be used to target specific days or customer segments for promotional activities, leading to increased sales and revenue.\n",
        "* The 'IsIncreasing' column helps identify the combinations where sales are consistently increasing. This knowledge can be leveraged to strengthen marketing strategies and promotional offers during those days, potentially driving even higher sales.\n",
        "\n",
        "**Negative Growth:**\n",
        "\n",
        "* From the output, all values in the 'IsIncreasing' column are marked as True, indicating that sales are increasing for all combinations of 'Customers', 'Promo', and 'DayOfWeek'. However, this might be a limitation of the provided dataset or the specific analysis conducted.\n",
        "* To identify negative growth or areas of improvement, we would need additional data or more sophisticated analysis. It's possible that certain combinations or time periods are experiencing stagnant or declining sales, but this is not evident from the given output."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n",
        "\n",
        "# Bivariate Analysis - Sales vs. StateHoliday\n",
        "sns.barplot(data=merged_df, x='StateHoliday', y='Sales', palette='pastel', ci=None, ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Sales vs. StateHoliday')\n",
        "axes[0, 0].set_xlabel('StateHoliday')\n",
        "axes[0, 0].set_ylabel('Sales')\n",
        "\n",
        "# Bivariate Analysis - Sales vs. SchoolHoliday\n",
        "sns.barplot(data=merged_df, x='SchoolHoliday', y='Sales', palette='Set2', ci=None, ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Sales vs. SchoolHoliday')\n",
        "axes[0, 1].set_xlabel('SchoolHoliday')\n",
        "axes[0, 1].set_ylabel('Sales')\n",
        "\n",
        "# Bivariate Analysis - Sales vs. StoreType\n",
        "sns.barplot(data=merged_df, x='StoreType', y='Sales', palette='tab10', ci=None, ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Sales vs. StoreType')\n",
        "axes[1, 0].set_xlabel('StoreType')\n",
        "axes[1, 0].set_ylabel('Sales')\n",
        "\n",
        "# Bivariate Analysis - Sales vs. Assortment\n",
        "sns.barplot(data=merged_df, x='Assortment', y='Sales', palette='Set1', ci=None, ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Sales vs. Assortment')\n",
        "axes[1, 1].set_xlabel('Assortment')\n",
        "axes[1, 1].set_ylabel('Sales')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In each case, the bar plots display the average sales for each category of the respective variable, making it easy to compare the sales across different groups. The choice of colors and style is done to enhance the visualization and differentiate between the categories. Overall, bar plots are suitable for this type of analysis when dealing with categorical variables and can provide valuable insights into how sales are influenced by these variables."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StateHoliday: The sales for stores on State Holidays (StateHoliday = 1.0) is relatively lower (258.64) compared to non-holidays (StateHoliday = 0.0) which have higher mean sales (5945.92).\n",
        "\n",
        "SchoolHoliday: The sales for stores on School Holidays (SchoolHoliday = 1.0) is higher (6474.89) compared to non-holidays (SchoolHoliday = 0.0) which have lower mean sales (5619.54).\n",
        "\n",
        "StoreType: The sales for stores of StoreType b is significantly higher (10058.84) compared to StoreType a (5736.60) and StoreType c (5723.63). StoreType d has a mean sales of 5639.35.\n",
        "\n",
        "Assortment: The sales for stores with Assortment type b is the highest (8553.93) compared to Assortment type a (5479.04) and Assortment type c (6057.87)."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the above chart can indeed help in creating a positive business impact and inform decision-making. Let's explore how each insight can impact the business:\n",
        "\n",
        "StateHoliday: The lower sales on State Holidays compared to non-holidays could be an opportunity for businesses to offer special promotions and discounts on these days to increase customer footfall and boost sales. By leveraging State Holidays effectively, businesses can attract more customers and drive higher sales during these periods.\n",
        "\n",
        "SchoolHoliday: The higher sales on School Holidays suggest that families and students are more likely to visit stores and make purchases during these times. Businesses can capitalize on this trend by running targeted marketing campaigns and offering promotions aimed at families and students. Additionally, businesses can optimize their inventory and staffing levels to meet the increased demand during School Holidays.\n",
        "\n",
        "StoreType: The significant difference in sales between different StoreTypes indicates that certain store formats (such as StoreType b) are more successful in generating sales compared to others. Businesses can use this insight to identify the strengths of each StoreType and replicate successful strategies across their stores. For example, they can implement marketing tactics or store layouts that have proven to be effective in driving sales in StoreType b.\n",
        "\n",
        "Assortment: The highest sales for stores with Assortment type b indicate that customers are more attracted to a specific assortment mix. Businesses can tailor their product offerings to align with the preferences of customers seeking Assortment type b. This could involve increasing the variety of products or categories that fall under Assortment type b to cater to customer demands."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "#Question-7 : what is average number of customers per day of week?\n",
        "\n",
        "axis = merged_df.groupby('DayOfWeek')[['Customers']].mean().plot(figsize = (10,5), marker = '^', color = 'b')\n",
        "axis.set_title('Average number of customers per day of the week')"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked the line plot because it is an effective way to show trends or changes in a numerical value (in this case, the average number of customers) over a continuous variable (in this case, the days of the week). The markers ('^') indicate individual data points for each day, and the line connects these points to show the overall trend. The use of color ('blue') makes the plot visually appealing and easy to interpret."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " the analysis reveals that Mondays tend to have the highest footfall, possibly because it is the start of the workweek, and customers may be more active in making purchases after the weekend. Sundays, on the other hand, have significantly lower footfall, which may be due to many businesses being closed or operating with reduced hours on Sundays. The insights gained from this analysis can be used to optimize staffing levels and promotional strategies for different days of the week to cater to the varying customer demand patterns."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "1. Staffing Optimization: With the knowledge of the days with the highest and lowest average customer counts, businesses can optimize their staffing levels accordingly. They can allocate more staff on Mondays and other high footfall days to ensure better customer service and efficiency.\n",
        "\n",
        "2. Promotional Strategies: Businesses can tailor their promotional strategies based on the customer demand patterns throughout the week. For example, they can plan special promotions or offers on days with lower footfall (e.g., Sundays) to attract more customers and boost sales.\n",
        "\n",
        "3. Resource Allocation: By understanding the customer traffic patterns on different days, businesses can efficiently allocate their resources such as inventory, marketing efforts, and operational activities to ensure maximum returns.\n",
        "\n",
        "**Negative Growth Insights (Areas of Concern):**\n",
        "\n",
        "1. Low Footfall on Sundays: The significant drop in average customer count on Sundays (approximately 36 customers) indicates a potential area of concern. This could be due to several factors such as limited store operating hours or reduced customer interest in shopping on Sundays. Businesses might need to evaluate whether it is financially viable to remain open on Sundays or if there are opportunities to attract more customers on this day.\n",
        "\n",
        "2. Decline in Footfall during the Week: While Mondays have the highest average customer count, there is a gradual decline in footfall throughout the week. If this trend indicates a consistent drop in customer visits, businesses might need to analyze the underlying reasons. They could consider implementing targeted marketing campaigns or introducing special weekday promotions to entice more customers to visit their stores during mid-week days."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "#Question- 8: average number of customers on stateholiday and schoolholiday?\n",
        "# Choose a color palette\n",
        "color_palette = 'Set2'  # You can choose any other color palette from Seaborn\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Violin plot for customers vs. SchoolHoliday\n",
        "sns.violinplot(data=merged_df, x='SchoolHoliday', y='Customers', palette=color_palette, ax=axes[0])\n",
        "axes[0].set_title('Customers vs. SchoolHoliday')\n",
        "axes[0].set_xlabel('SchoolHoliday')\n",
        "axes[0].set_ylabel('Number of Customers')\n",
        "\n",
        "# Violin plot for customers vs. StateHoliday\n",
        "sns.violinplot(data=merged_df, x='StateHoliday', y='Customers', palette=color_palette, ax=axes[1])\n",
        "axes[1].set_title('Customers vs. StateHoliday')\n",
        "axes[1].set_xlabel('StateHoliday')\n",
        "axes[1].set_ylabel('Number of Customers')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xnAtWez5-tDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " violin plots are useful when we have categorical variables and want to visualize the distribution of a numerical variable across those categories. They provide a concise summary of the data distribution and allow for easy comparisons between different groups. Additionally, the use of color palettes in the violin plot makes it visually appealing and aids in highlighting the differences between the categories."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Average number of customers on State Holidays: The average number of customers on State Holidays is 40.13. This indicates that there is a significant decrease in customer footfall on State Holidays compared to regular days.\n",
        "\n",
        "2. Average number of customers on Non-State Holidays: On regular days (Non-State Holidays), the average number of customers is higher at 651.84. This suggests that customers tend to visit the stores more on normal days when there are no State Holidays.\n",
        "\n",
        "3. Average number of customers on School Holidays: The average number of customers on School Holidays is 704.44. This indicates that there is a slight increase in customer footfall on School Holidays compared to regular days.\n",
        "\n",
        "4. Average number of customers on Non-School Holidays: On regular days (Non-School Holidays), the average number of customers is slightly lower at 617.67. This suggests that customers may visit the stores slightly less on normal days when there are no School Holidays.\n",
        "\n",
        "Overall, these insights suggest that State Holidays have a more significant impact on reducing customer footfall compared to School Holidays. On normal days without any holidays (both State and School), the average number of customers is higher. This information could be useful for businesses to plan their staffing, inventory, and promotional strategies around holidays to better serve their customers' needs and maximize sales opportunities."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "1. Optimizing Staffing: Understanding the average number of customers on State Holidays and School Holidays allows businesses to adjust their staffing levels accordingly. On State Holidays, where footfall is lower, they can operate with a reduced workforce to optimize labor costs. Conversely, on School Holidays, when there is a slight increase in footfall, they can have more staff to handle higher customer traffic efficiently.\n",
        "\n",
        "2. Promotional Strategies: Businesses can tailor their promotional strategies based on customer behavior during holidays. For example, during State Holidays with lower customer footfall, they may offer more attractive promotions and discounts to attract shoppers. On the other hand, during School Holidays when there is increased footfall, they might focus on upselling or cross-selling to capitalize on the higher customer traffic.\n",
        "\n",
        "3. Inventory Management: Understanding customer behavior during holidays helps businesses plan their inventory better. On State Holidays, they can reduce stock levels for items with lower demand, avoiding excess inventory. Conversely, during School Holidays, they can ensure sufficient stock for popular products to meet increased demand."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "#Question- 9: what is total number of customers in each storetype and share of each storetype in total stores type?\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 6))\n",
        "\n",
        "# Share of Store Types\n",
        "store_type_counts = merged_df[\"StoreType\"].value_counts()\n",
        "axes[0].pie(store_type_counts, labels=store_type_counts.index, autopct='%1.1f%%', shadow=True)\n",
        "axes[0].set_title('Share of Store Types')\n",
        "\n",
        "# Customer Share by Store Type\n",
        "customer_by_store_type = merged_df.groupby('StoreType')['Customers'].sum()\n",
        "axes[1].pie(customer_by_store_type, labels=customer_by_store_type.index, autopct='%1.1f%%', shadow=True)\n",
        "axes[1].set_title('Customer Share by Store Type')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pie charts are useful for displaying parts-to-whole relationships and are particularly effective when you want to highlight the contribution of individual categories to a total or compare the relative sizes of different categories. They provide an easy-to-understand representation of data and are visually appealing."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Store Type a has the highest percentage of customers, accounting for the majority of the total customers and highest number of shares too."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "* Understanding the distribution of customers across different store types allows businesses to focus their marketing efforts and tailor promotions to the most significant customer segments. For instance, since Store Type a has the highest share of customers, targeted marketing campaigns can be designed specifically for this customer group to enhance customer loyalty and retention.\n",
        "* Recognizing that Store Type d has a considerable share of both customers and total store types, businesses can assess the success and performance of Store Type d locations compared to other types. Identifying the factors contributing to the popularity of Store Type d can help replicate successful strategies in other store types, driving overall growth.\n",
        "\n",
        "**Negative Growth Considerations:**\n",
        "\n",
        "* The lower percentage of customers and store types in Store Type b (4.89% and 1.56%, respectively) suggests that this store type may be underperforming in terms of attracting customers and expanding its presence. Further investigation into the reasons behind this lower share is essential to identify potential issues and devise strategies to boost growth.\n",
        "* The share of Store Type c in both customers (14.33%) and total store types (13.48%) indicates moderate performance compared to Store Types a and d. While not indicating negative growth, this insight may prompt businesses to assess the competitive landscape and identify areas for improvement to strengthen Store Type c's position."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "jq-YJGHDp576"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "#Question-10: What is the correlation between sales and the competition distance?\n",
        "# Create a scatter plot to visualize the relationship between Sales and CompetitionDistance\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(data=sales_competition_data, x='CompetitionDistance', y='Sales')\n",
        "plt.xlabel('Competition Distance')\n",
        "plt.ylabel('Sales')\n",
        "plt.title(f'Correlation between Sales and Competition Distance\\nCorrelation Coefficient: {correlation_coefficient}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eofEvGelp577"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "YW8aNWAQp579"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot allows us to visualize the pattern of data points and identify any potential correlation between sales and competition distance. The correlation coefficient gives us a numerical measure of the strength and direction of the relationship. A positive correlation coefficient indicates a positive relationship, while a negative coefficient indicates a negative relationship. A value close to 0 suggests a weak or no correlation."
      ],
      "metadata": {
        "id": "21CYLvF6p57-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "yuFsdoKmp57-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a correlation coefficient value of -0.0189 indicates a weak or negligible linear relationship between sales and competition distance in the dataset. For business decision-making, it suggests that competition distance alone may not be a strong predictor of sales performance, and other factors might have a more significant impact on sales in the retail stores."
      ],
      "metadata": {
        "id": "dRSjrpVXp57_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "iqoKbMRap58A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the weak correlation between sales and competition distance highlights the importance of considering a holistic approach to sales performance improvement. While competition distance is one of the many factors that can influence sales, relying solely on this factor might not lead to significant business impact. Instead, businesses should adopt a comprehensive strategy, focusing on customer preferences, competitive advantage, and store-specific factors to drive positive growth."
      ],
      "metadata": {
        "id": "yW4XSoOmp58B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-eWqMmPtp4xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "#Question- 11: how sales are varying per day in a month and varying per month in a year?\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Sales vs. Day of Month\n",
        "sns.lineplot(data=merged_df, x='DayOfMonth', y='Sales', ax=axes[0])\n",
        "axes[0].set_title('Sales vs. Day of Month')\n",
        "axes[0].set_xlabel('Day of Month')\n",
        "axes[0].set_ylabel('Sales')\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Sales vs. Month\n",
        "sns.lineplot(data=merged_df, x='Month', y='Sales', ax=axes[1])\n",
        "axes[1].set_title('Sales vs. Month')\n",
        "axes[1].set_xlabel('Month')\n",
        "axes[1].set_ylabel('Sales')\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of Sales vs. Day of Month, a line plot helps to visualize any patterns or trends in sales values over the course of the month, such as whether there are any peaks or drops in sales on specific days.\n",
        "\n",
        "For Sales vs. Month, a line plot helps to show the overall trend in sales across different months, allowing us to identify any seasonal patterns or fluctuations in sales throughout the year."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Sales by Day of Month: The average daily sales vary throughout the month. We can observe fluctuations in sales values across different days of the month. For example, the average sales on the 30th day of the month (7295.48) is relatively higher compared to other days, whereas the average sales on the 25th (4822.33) and 26th (4835.85) days are lower.\n",
        "\n",
        "2. Sales by Month: The average sales values also show variation across different months. For instance, the months of December (6824.83) and July (6063.74) have relatively higher average sales, while January (5463.71) and May (5488.73) have lower average sales.\n",
        "\n",
        "3. Seasonal Patterns: From the sales by month data, we can observe that sales tend to be higher in December (holiday season) and lower in January and February. This indicates a seasonal pattern, which could be attributed to increased consumer spending during the holiday season.\n",
        "\n",
        "4. Daily Fluctuations: The sales by day of month data indicates that there are fluctuations in sales values on specific days of the month. These fluctuations could be influenced by factors like weekends, paydays, or promotional activities."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Business Impact:**\n",
        "\n",
        "1. Seasonal Planning: Understanding the seasonal patterns can help businesses prepare for high-demand periods (e.g., holidays) and ensure sufficient stock and workforce availability to meet customer demands. This proactive approach can lead to increased sales and customer satisfaction.\n",
        "\n",
        "2. Targeted Promotions: Knowing the fluctuations in sales on specific days of the month can enable businesses to target promotions and marketing efforts more effectively. For example, offering discounts or special offers on days with historically lower sales can attract more customers and boost revenue.\n",
        "\n",
        "3. Resource Allocation: With insights into sales variations, businesses can allocate resources more efficiently, such as scheduling staff accordingly based on customer footfall trends, optimizing inventory levels, and managing operational costs.\n",
        "\n",
        "**Negative Growth Mitigation:**\n",
        "\n",
        "1. Addressing Low-Sales Periods: By identifying months or days with lower sales, businesses can plan strategies to mitigate the negative impact on revenue. For example, they can implement cost-saving measures during these periods or introduce creative marketing campaigns to attract more customers.\n",
        "\n",
        "2. Inventory Management: Understanding sales patterns can help businesses avoid overstocking during periods of slow sales, reducing the risk of inventory wastage and associated costs.\n",
        "\n",
        "3. Adapting Business Strategies: Businesses can use the insights to adapt their business strategies based on changing customer behaviors or external factors affecting sales, allowing them to stay competitive and resilient in challenging market conditions."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "#Question- 12 : Average Sales Per Year?\n",
        "\n",
        "# Plot the Sales vs. Year using a lineplot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.lineplot(data=sales_by_year, x='Year', y='Sales', marker='o', color='b')\n",
        "plt.title('Average Sales vs. Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a line plot for Sales vs. Year because it is suitable for visualizing trends and changes in a continuous variable (Sales) over time (Year). Line plots are particularly effective when we want to observe how a variable changes across different time periods or numerical values."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sales Trend: The average sales show a gradual upward trend over the three years. The sales increased from 2013 to 2014 and continued to rise in 2015. This suggests that the business is experiencing growth and attracting more customers over time.\n",
        "\n",
        "Seasonal Variations: While the overall trend is positive, there might be seasonal variations within each year. For example, sales might peak during certain months or quarters and dip during others. Further analysis would be required to identify and understand these seasonal patterns."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights can help create a positive business impact. The analysis reveals that the average sales have shown a gradual upward trend over the years (from 2013 to 2015). This positive trend indicates that the business is experiencing growth and attracting more customers, which can have several positive implications:\n",
        "\n",
        "* Business Growth: The increasing average sales suggest that the business is expanding and attracting more customers. This growth can lead to higher revenues and profits, creating a positive impact on the company's financial performance.\n",
        "\n",
        "* Customer Attraction: The upward trend in sales indicates that the business is successful in attracting and retaining customers. This can be a positive sign for the company's reputation and brand image.\n",
        "\n",
        "* Market Demand: The increasing sales over the years can also signify that there is a growing demand for the company's products or services in the market. This insight can guide the business to focus on meeting customer needs and preferences."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "#correlation heatmap\n",
        "plt.figure(figsize=(16,10))\n",
        "sns.heatmap(corr_df.corr(), cmap=\"coolwarm\", annot=True)"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A heatmap is a useful visualization for quickly identifying patterns of correlation between numerical features in the dataset. It can help in understanding which features have a stronger relationship with each other and may be useful for further analysis or modeling."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**few insights are -**\n",
        "\n",
        "\n",
        "* There is a strong positive correlation between Sales and Customers (correlation coefficient close to 1.00). This indicates that as the number of customers increases, the sales also tend to increase, which is expected and makes sense. Higher footfall leads to higher sales.\n",
        "\n",
        "\n",
        "* There is a positive correlation between Sales and Promo, but the correlation is not very strong (correlation coefficient around 0.38). This suggests that promotional activities (Promo) have some positive impact on sales, but other factors also play a significant role.\n",
        "\n",
        "\n",
        "* There is a weak negative correlation between Sales and CompetitionDistance (correlation coefficient around -0.12). This indicates that stores located closer to their competitors tend to have slightly lower sales. It's important to note that this correlation is weak, so other factors may have a more significant impact on sales."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "#Create a pairplot to check correlation between all important features like Sales, Customers, Promo, CompetitionDistance etc.\n",
        "\n",
        "\n",
        "# Subset the features for the pairplot\n",
        "subset_features = ['Sales', 'Customers', 'Promo', 'CompetitionDistance', 'Month','Year']\n",
        "\n",
        "# Create a pairplot for the selected features\n",
        "sns.pairplot(merged_df[subset_features])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is used to create a grid of scatter plots that shows the pairwise relationships between multiple variables in a dataset. It is a useful tool for exploring the correlations between variables in a dataset and identifying any patterns or trends that may exist.\n",
        "\n",
        "Using pairplot can help to identify potential areas of interest for further analysis and may provide valuable insights that can inform decision-making in the app development and marketing processes."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Sales and Customers:\n",
        "\n",
        "The scatter plot between Sales and Customers shows a positive linear relationship. As the number of customers increases, the sales tend to increase as well. This aligns with the high positive correlation between Sales and Customers that we observed in the correlation heatmap.\n",
        "\n",
        "* Sales and Open:\n",
        "\n",
        "The plot between Sales and Open shows that when the store is open (Open=1), there are higher sales. This makes sense because when the store is closed (Open=0), sales would naturally be zero or significantly lower.\n",
        "\n",
        "* Sales and CompetitionDistance:\n",
        "\n",
        "There is no strong linear pattern observed between Sales and CompetitionDistance. The scatter plot indicates a weak correlation, which is consistent with the weak negative correlation observed in the correlation heatmap. The impact of competition distance on sales is not very significant based on this plot.\n",
        "\n",
        "* Sales and Month:\n",
        "\n",
        "The pair plot between Sales and Month shows the average sales for each month. It appears that sales vary throughout the year, with some months experiencing higher sales compared to others. This aligns with the previous output that showed the average sales for each month.\n",
        "\n",
        "* Sales and Year:-\n",
        "\n",
        "The Pair plot between Sales and Year shows the average sales for each year. It indicates that sales have increased over the years, with the highest average sales observed in 2015. This corresponds to the previous output that showed the average sales for each year."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statement 1: The average sales on school holidays are higher than the average sales on non-school holidays.\n",
        "\n",
        "Statement 2: The sales for stores with StoreType 'b' are significantly higher than the sales for stores with StoreType 'a'.\n",
        "\n",
        "Statement 3: There is a significant positive correlation between the number of customers and sales."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): μSH ≤ μNSH (The average sales on school holidays are less than or equal to the average sales on non-school holidays.)\n",
        "\n",
        "Alternative Hypothesis (Ha): μSH > μNSH (The average sales on school holidays are higher than the average sales on non-school holidays.)"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Filter the data for school holidays and non-school holidays\n",
        "sales_school_holiday = merged_df[merged_df['SchoolHoliday'] == 1]['Sales']\n",
        "sales_non_school_holiday = merged_df[merged_df['SchoolHoliday'] == 0]['Sales']\n",
        "\n",
        "# Perform the t-test\n",
        "t_stat, p_value = stats.ttest_ind(sales_school_holiday, sales_non_school_holiday, alternative='greater')\n",
        "\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The t-statistic is a measure of how many standard deviations the sample mean of the school holiday sales is away from the sample mean of the non-school holiday sales. In this case, the t-statistic is 86.07, which is a large value, indicating a significant difference between the means of the two groups.\n",
        "\n",
        "The p-value is a measure of the probability of obtaining the observed t-statistic or more extreme values if the null hypothesis (μSH ≤ μNSH) were true. In this case, the p-value is reported as 0.0, which means that the probability of obtaining the observed difference in means due to random chance is effectively zero.\n",
        "\n",
        "With such a small p-value, we can reject the null hypothesis in favor of the alternative hypothesis (μSH > μNSH), providing strong evidence to support that the average sales on school holidays are indeed higher than the average sales on non-school holidays.\n"
      ],
      "metadata": {
        "id": "xvNFz0MFZLsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have performed an independent t-test to obtain the p-value for Statement 1."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the independent t-test because it is suitable for comparing the means of two independent groups, in this case, the average sales on school holidays and non-school holidays. The independent t-test assumes that the samples are independent, normally distributed, and have equal variances."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): μStoreType_b ≤ μStoreType_a (The average sales for stores with StoreType 'b' are less than or equal to the average sales for stores with StoreType 'a'.)\n",
        "\n",
        "Alternative Hypothesis (Ha): μStoreType_b > μStoreType_a (The average sales for stores with StoreType 'b' are higher than the average sales for stores with StoreType 'a'.)"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Extract the sales data for stores with StoreType 'b'\n",
        "sales_store_type_b = merged_df[merged_df['StoreType'] == 'b']['Sales']\n",
        "\n",
        "# Extract the sales data for stores with StoreType 'a'\n",
        "sales_store_type_a = merged_df[merged_df['StoreType'] == 'a']['Sales']\n",
        "\n",
        "# Perform the independent t-test\n",
        "t_statistic, p_value = stats.ttest_ind(sales_store_type_b, sales_store_type_a, alternative='greater')\n",
        "\n",
        "# Output the results\n",
        "print(\"t-statistic:\", t_statistic)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-statistic: The t-statistic is a measure of how many standard deviations the sample mean of StoreType 'b' is away from the sample mean of StoreType 'a'. In this case, the t-statistic is 133.818, which indicates a large difference in the means of the two groups.\n",
        "\n",
        "p-value: The p-value is a measure of the probability of obtaining the observed results or more extreme results if the null hypothesis is true. In this case, the p-value is 0.0, which means there is essentially no chance of observing such a large difference in sales between StoreType 'b' and StoreType 'a' if the null hypothesis (that there is no difference in means) is true.\n",
        "\n",
        "Based on these insights, we can reject the null hypothesis (H0: μStoreType_b ≤ μStoreType_a) in favor of the alternative hypothesis (Ha: μStoreType_b > μStoreType_a). This means that there is a significant difference in sales between stores with StoreType 'b' and StoreType 'a', and the sales for stores with StoreType 'b' are significantly higher than the sales for stores with StoreType 'a'."
      ],
      "metadata": {
        "id": "7ttmHfvogVkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have performed an independent two-sample t-test to obtain the p-value. The independent t-test is used to compare the means of two independent groups (StoreType 'b' and StoreType 'a') and determine whether the difference in means is statistically significant. The obtained p-value from the t-test helps in making this determination."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the independent two-sample t-test for Statement 2 because it is appropriate for comparing the means of two independent groups (StoreType 'b' and StoreType 'a'). The t-test is commonly used when we have two groups, and we want to determine if there is a statistically significant difference between their means.\n",
        "\n",
        "In this case, we are comparing the sales of two different store types ('b' and 'a'). We want to test if the average sales of stores with StoreType 'b' are significantly higher than the average sales of stores with StoreType 'a'. The t-test allows us to compare the means of these two groups and determine if the observed difference in sales is statistically significant."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no correlation between the number of customers and sales. (ρ = 0)\n",
        "\n",
        "Alternative Hypothesis (Ha): There is a significant positive correlation between the number of customers and sales. (ρ > 0)"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Calculate the Pearson correlation coefficient and p-value\n",
        "correlation_coefficient, p_value = stats.pearsonr(merged_df['Customers'], merged_df['Sales'])\n",
        "\n",
        "print(\"Correlation Coefficient:\", correlation_coefficient)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation coefficient measures the strength and direction of the linear relationship between two variables. In this case, the correlation coefficient is 0.8947, which indicates a strong positive linear relationship between the number of customers and sales. This suggests that as the number of customers increases, the sales also tend to increase.\n",
        "\n",
        "The p-value is reported as 0.0, which means it is very close to zero. In statistical hypothesis testing, a p-value less than the significance level (commonly set at 0.05) suggests strong evidence against the null hypothesis. In this case, since the p-value is extremely small, we reject the null hypothesis, and the result is statistically significant.\n",
        "\n",
        "In conclusion, based on the Pearson correlation coefficient test, there is a significant positive correlation (r = 0.8947) between the number of customers and sales in the dataset. This implies that stores with higher customer counts tend to have higher sales."
      ],
      "metadata": {
        "id": "9Uan8xFuizs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Pearson correlation coefficient test to obtain the p-value for Statement 3. The Pearson correlation coefficient measures the linear relationship between two continuous variables. It is suitable for determining whether there is a significant positive or negative correlation between the number of customers and sales in our dataset."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason for choosing the Pearson correlation coefficient test is that both 'Customers' and 'Sales' are continuous variables, and we want to understand if there is a linear relationship between them. By calculating the correlation coefficient and its associated p-value, we can determine whether there is a significant correlation between the number of customers and sales."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Check for missing values in each column of the merged dataset\n",
        "missing_values_count = merged_df.isnull().sum()\n",
        "print(missing_values_count)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see there is no missing value left in this merged dataset because we already treat them before doing EDA process."
      ],
      "metadata": {
        "id": "jk5HQZc2jwn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used median and mode for missing value imputation.\n",
        "\n",
        "`Median Imputation`:- It Replace missing values with the median (for skewed data) of the available values in the column.I Used This method Because It is suitable for numerical data with no significant outliers. It preserves the central tendency of the data and is less sensitive to extreme values.\n",
        "\n",
        "`Mode Imputation`:- It Replace missing values with the mode (most frequent value) of the available values in the column (for categorical data). I use it Because Mode imputation works well for categorical data with a low number of unique categories. It preserves the most common category distribution."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Select only numerical columns for box plot visualization\n",
        "numerical_cols = merged_df.select_dtypes(include='number').columns\n",
        "\n",
        "# Create box plots for numerical columns to visualize potential outliers\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(data=merged_df[numerical_cols])\n",
        "plt.title(\"Box Plot for Numerical Columns (to identify outliers)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we will calculate percentage of outliers in each features after that we will handle outlier in respective features."
      ],
      "metadata": {
        "id": "kFBMuF9Skkq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Identify numerical columns with potential outliers\n",
        "numerical_cols = merged_df.select_dtypes(include='number').columns\n",
        "\n",
        "# Set the z-score threshold for identifying outliers\n",
        "z_score_threshold = 3\n",
        "\n",
        "# Dictionary to store the percentage of outliers for each numerical column\n",
        "percentage_of_outliers = {}\n",
        "\n",
        "# Loop through each numerical column and calculate the percentage of outliers\n",
        "for col in numerical_cols:\n",
        "    col_mean = merged_df[col].mean()\n",
        "    col_std = merged_df[col].std()\n",
        "    z_scores = np.abs((merged_df[col] - col_mean) / col_std)\n",
        "    num_outliers = len(merged_df[z_scores > z_score_threshold])\n",
        "    percentage = (num_outliers / len(merged_df)) * 100\n",
        "    percentage_of_outliers[col] = percentage\n",
        "\n",
        "# Print the percentage of outliers for each numerical column\n",
        "for col, percentage in percentage_of_outliers.items():\n",
        "    print(f\"Percentage of outliers in {col}: {percentage:.2f}%\")\n"
      ],
      "metadata": {
        "id": "7X2dZSLClYB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As sales and customers are having a strong correlation and they are very important features here thats why removing their outliers will not a good idea as it could affect our model quality so we will Removing outliers From StateHoliday, CompetitionDistance And CompetitionOpenSinceYear."
      ],
      "metadata": {
        "id": "-NazljKtmDzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the features for which outliers will be removed\n",
        "features_with_outliers = [ 'StateHoliday', 'CompetitionDistance', 'CompetitionOpenSinceYear']\n",
        "\n",
        "# Set the z-score threshold for identifying outliers\n",
        "z_score_threshold = 3\n",
        "\n",
        "# Loop through each feature and remove rows with outliers based on z-scores\n",
        "for feature in features_with_outliers:\n",
        "    feature_mean = merged_df[feature].mean()\n",
        "    feature_std = merged_df[feature].std()\n",
        "    z_scores = np.abs((merged_df[feature] - feature_mean) / feature_std)\n",
        "    merged_df = merged_df[z_scores <= z_score_threshold]\n"
      ],
      "metadata": {
        "id": "I_lY6DfkndP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "pVOZrr_IlUWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we used the z-score method for outlier treatment. Z-score is a widely used technique to identify and handle outliers in a dataset. It helps us understand how far each data point is from the mean in terms of standard deviations.\n",
        "\n",
        "Here's why we used the z-score method for outlier treatment:\n",
        "\n",
        "**1. Z-Score Method**: The z-score method is suitable when dealing with data that is approximately normally distributed. It helps to identify extreme values (outliers) that are significantly different from the rest of the data in terms of standard deviations. By setting a threshold (e.g., z-score threshold of 3), we can determine which data points are outliers and subsequently handle them based on the analysis goals.\n",
        "\n",
        "**2. Applicability to Numerical Data**: The z-score method is well-suited for numerical data as it requires computing the mean and standard deviation, which are meaningful statistical measures for numerical features.\n",
        "\n",
        "**3. Robustness to Scale**: The z-score method is robust to the scale of the data, as it standardizes each data point based on the mean and standard deviation. This allows us to compare and identify outliers in features with different units and scales.\n",
        "\n",
        "**4. Easy Implementation**: The z-score method is straightforward to implement and is readily available in most data analysis libraries, such as NumPy and Pandas."
      ],
      "metadata": {
        "id": "ehCVOLXjlUWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# One-hot encoding for StoreType column\n",
        "merged_df = pd.get_dummies(merged_df, columns=['StoreType'], drop_first=True)\n",
        "# One-hot encoding for Assortment column\n",
        "merged_df = pd.get_dummies(merged_df, columns=['Assortment'], drop_first=True)\n",
        "# One-hot encoding for PromoInterval column\n",
        "merged_df = pd.get_dummies(merged_df, columns=['PromoInterval'], drop_first=True)\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, I used one-hot encoding for the categorical columns in the stores_df dataset. One-hot encoding is a popular technique to convert categorical variables into a binary representation, making them suitable for machine learning algorithms that expect numerical inputs.\n",
        "\n",
        "\n",
        "Reason For Choosing One-Hot Encoding:-\n",
        "\n",
        "One-hot encoding is used when the categorical variable has no ordinal relationship. It creates binary columns for each category, representing the presence or absence of the category in the original data. This approach is suitable for nominal categorical variables, where there is no natural order between categories.\n",
        "One-hot encoding ensures that the machine learning models can understand and process the categorical information correctly. It prevents any ordinal relationship from being imposed on the categories, which might not be appropriate for certain models."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "qvMRdAjP-f4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "GxMRQFVa-f5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets remove store since we need sales of all stores, not a particular one and also sales can be predicted through store type, assortment, etc."
      ],
      "metadata": {
        "id": "CfPoDsUW1FI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# drop Store\n",
        "merged_df.drop('Store', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "LUvsQDRs-f5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N6K4Ml0cIRr"
      },
      "source": [
        "Lets remove date since there are already day of week and week of year features in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXyxq--ecIRr"
      },
      "outputs": [],
      "source": [
        "# drop Date\n",
        "merged_df.drop('Date', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lG34OGPcbLw"
      },
      "source": [
        "Lets remove competition open since month and competition open since year as the information provided by them can be obtained from competition open number of months."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfbyPuHfd36q"
      },
      "outputs": [],
      "source": [
        "# drop CompetitionOpenSinceMonth & CompetitionOpenSinceYear\n",
        "merged_df.drop(['CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6pQLzqMdqZP"
      },
      "source": [
        "Lets remove promo 2, promo 2 since week and promo 2 since year as the information provided by them can be obtained from promo 2 number of weeks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kw9snCWmeI30"
      },
      "outputs": [],
      "source": [
        "# drop Promo2, Promo2SinceWeek & Promo2SinceYear\n",
        "merged_df.drop(['Promo2', 'Promo2SinceWeek', 'Promo2SinceYear'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where Sales is equal to 0\n",
        "merged_df = merged_df.loc[merged_df['Sales'] != 0]\n",
        "# Drop rows where Customers is equal to 0\n",
        "merged_df = merged_df.loc[merged_df['Customers'] != 0]\n",
        "#no of observations for closed stores with 0 sales\n",
        "(merged_df[merged_df.Open == 0]).shape"
      ],
      "metadata": {
        "id": "jPV6_zrL2z4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#since the stores closed had 0 sale value; removing the irrelevant part\n",
        "meerged_df = merged_df[merged_df.Open != 0]\n",
        "merged_df.drop('Open', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "_j55xqcY3Dt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ES3CwZTsn5f"
      },
      "outputs": [],
      "source": [
        "# exploring the head of the resultant dataframe\n",
        "merged_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "lF6IPLZF-f5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "# Separate the feature matrix 'X' and the target variable 'y'\n",
        "X = merged_df.drop(columns=['Sales'])\n",
        "y = merged_df['Sales']\n",
        "\n",
        "# Number of top features to select\n",
        "k = 15\n",
        "\n",
        "# Perform feature selection using ANOVA\n",
        "selector = SelectKBest(score_func=f_regression, k=k)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Get the indices of the selected features\n",
        "selected_feature_indices = selector.get_support(indices=True)\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_feature_names = X.columns[selected_feature_indices]\n",
        "selected_feature_scores = selector.scores_[selected_feature_indices]\n",
        "# Now, 'X_selected' contains only the selected features, and 'selected_feature_names' contains their names.\n",
        "# Print the selected features and their corresponding ANOVA F-values\n",
        "print(\"Selected Features:\")\n",
        "for feature, score in zip(selected_feature_names, selected_feature_scores):\n",
        "    print(f\"{feature}: ANOVA F-value = {score}\")"
      ],
      "metadata": {
        "id": "Px2qd1yD-f5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code for feature selection using SelectKBest with ANOVA, we selected the top 'k' features based on their statistical significance in relation to the target variable 'Sales'. The higher the ANOVA F-value for a feature, the more important it is in explaining the variation in the target variable.\n",
        "\n",
        "After running the feature selection code, we can access the selected features and their corresponding ANOVA F-values using the scores_ attribute of the SelectKBest object."
      ],
      "metadata": {
        "id": "Ia02jJY8CjjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "aUNNTdN6-f5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code for feature selection in the merged dataset, we used the SelectKBest method with the ANOVA (Analysis of Variance) score function. Let's discuss the feature selection methods used and the reasons for choosing them:\n",
        "\n",
        "SelectKBest with ANOVA: SelectKBest is a feature selection method from scikit-learn that selects the top 'k' features based on univariate statistical tests. The ANOVA score function is used specifically for regression tasks (predicting continuous target variables) and evaluates the relationship between each feature and the target variable using ANOVA F-values.\n",
        "\n",
        "Reason for using SelectKBest with ANOVA: We chose this method because the target variable 'Sales' is a continuous numerical variable in the regression task. The ANOVA F-values help us assess the statistical significance of each feature's relationship with the target. By selecting the top 'k' features, we aim to keep the most informative features and reduce the model's complexity, which can help prevent overfitting."
      ],
      "metadata": {
        "id": "9z2IvVhZ-f5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "_nsKino0-f5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DayOfWeek: The day of the week may impact sales due to different consumer behavior on different days.\n",
        "\n",
        "Customers: The number of customers visiting the store is a critical factor influencing sales.\n",
        "\n",
        "Promo: Whether there is a promotion on a particular day can significantly affect sales.\n",
        "\n",
        "SchoolHoliday: Sales may be influenced during school holidays when families have more time for shopping.\n",
        "\n",
        "StoreType: Different types of stores (A, B, C, D) may have varying sales patterns.\n",
        "\n",
        "Assortment: The assortment of products offered by the store could affect sales.\n",
        "\n",
        "CompetitionDistance: The distance to the nearest competitor's store may impact sales.\n",
        "\n",
        "PromoInterval_Feb,May,Aug,Nov: Promotion during specific months could have a positive impact on sales.\n",
        "\n",
        "PromoInterval_Jan,Apr,Jul,Oct: Promotion during specific months could also impact sales.\n",
        "\n",
        "PromoInterval_Mar,Jun,Sept,Dec: Promotion during specific months could have different effects on sales.\n",
        "\n",
        "Based on the ANOVA F-values, the top 10 features listed above are considered more important in explaining the variability in 'Sales'."
      ],
      "metadata": {
        "id": "-5NwsQh4-f5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "merged_df['Sales'] = np.log(merged_df['Sales'])\n",
        "merged_df.drop(merged_df[merged_df['Sales'] == float(\"-inf\")].index,inplace=True)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why We Used**-\n",
        "Log transformation is commonly used on the 'Sales' (or any positive numeric) variable when it exhibits positive skewness or a long tail to the right in its distribution. Positive skewness means that the data is skewed towards higher values, and the long tail indicates that there are extreme values that are much larger than the majority of the data."
      ],
      "metadata": {
        "id": "27809m_wpFz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "\n",
        "# Create a copy of the DataFrame\n",
        "merged_df_scaled = merged_df.copy()\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_columns = merged_df.select_dtypes(include='number').columns\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply StandardScaler to all numeric columns\n",
        "merged_df_scaled[numeric_columns] = scaler.fit_transform(merged_df[numeric_columns])\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the StandardScaler from scikit-learn because it is a widely used method for feature scaling, especially when dealing with algorithms that rely on distance-based calculations or gradient-based optimization. Here's why I chose to use the StandardScaler:\n",
        "\n",
        "* Standardization: The StandardScaler scales the features in a way that they have a mean of 0 and a standard deviation of 1. This process is known as standardization or z-score scaling. Standardizing the features helps to center the data around zero, making it easier for certain algorithms to converge during training.\n",
        "\n",
        "* Preserves Shape of Distribution: The StandardScaler preserves the shape of the original distribution while scaling the features. It only shifts and scales the values, maintaining the relative relationships between data points."
      ],
      "metadata": {
        "id": "BBIast9CK0qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "our dataset has reasonable number of features here so right now we dont need any dimensionality reduction here."
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test_size parameter in the train_test_split function controls the proportion of the data that should be allocated to the testing set when splitting the dataset into training and testing sets. In the above code, test_size=0.2 is used, which means that 20% of the data will be allocated to the testing set, and the remaining 80% will be used for training.The commonly used splitting ratios are 80:20 (test_size=0.2) and 70:30 (test_size=0.3). These ratios strike a good balance between having enough data for training and obtaining a reliable evaluation on the testing set."
      ],
      "metadata": {
        "id": "-87CCA8CQjgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "oQOXryDxMRNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We will build four ML Models:\n",
        "\n",
        "\n",
        "*   Liniear Regression\n",
        "*   XGBoost\n",
        "*   Decision Tree\n",
        "*   Random Forest\n",
        "\n",
        "After building models we will evaluate their performance and select one which will give best results."
      ],
      "metadata": {
        "id": "efrb9Slpxfq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1-> Linear Regression"
      ],
      "metadata": {
        "id": "2nx60QQEMRNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# Fitting Multiple Linear Regression to the Training set\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predicting test values:\n",
        "y_pred = regressor.predict(X_test)\n",
        "y_pred\n",
        "# Predict on the model\n",
        "# After building the model we are comparing the actual and the predicted values in this code:\n",
        "\n",
        "data = pd.DataFrame({'Actual':y_test, 'Predicted':y_pred})\n",
        "data"
      ],
      "metadata": {
        "id": "c0AYibtSMRNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "73Lzkv3FMRNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Performance of the model\n",
        "\n",
        "r2s_1 = r2(y_test,y_pred)\n",
        "mae1 = mae(y_test,y_pred)\n",
        "rmse1 = math.sqrt(mse(y_test,y_pred))\n",
        "print('Performance of Linear Regression Model:')\n",
        "print('-'*40)\n",
        "print('r2_score:',r2s_1)\n",
        "print('Mean absolute error: %.2f' % mae1)\n",
        "print('Root mean squared error: ', rmse1)"
      ],
      "metadata": {
        "id": "MYuUJNjhMRN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "5ePUfo8eMRN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Hyperparameter tuning for Ridge Regression\n",
        "ridge_params = {'alpha': [0.1, 1.0, 10.0]}\n",
        "ridge_model = Ridge()\n",
        "ridge_grid = GridSearchCV(ridge_model, ridge_params, cv=5, scoring='neg_mean_squared_error')\n",
        "ridge_grid.fit(X_train, y_train)\n",
        "\n",
        "# Hyperparameter tuning for Lasso Regression\n",
        "lasso_params = {'alpha': [0.1, 1.0, 10.0]}\n",
        "lasso_model = Lasso()\n",
        "lasso_grid = GridSearchCV(lasso_model, lasso_params, cv=5, scoring='neg_mean_squared_error')\n",
        "lasso_grid.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters for Ridge and Lasso Regression\n",
        "best_ridge_alpha = ridge_grid.best_params_['alpha']\n",
        "best_lasso_alpha = lasso_grid.best_params_['alpha']\n",
        "\n",
        "# Create Ridge and Lasso Regression models with the best hyperparameters\n",
        "best_ridge_model = Ridge(alpha=best_ridge_alpha)\n",
        "best_lasso_model = Lasso(alpha=best_lasso_alpha)\n",
        "\n",
        "# Fit the models on the training data\n",
        "best_ridge_model.fit(X_train, y_train)\n",
        "best_lasso_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "ridge_y_pred = best_ridge_model.predict(X_test)\n",
        "lasso_y_pred = best_lasso_model.predict(X_test)\n",
        "\n",
        "# Evaluate the models\n",
        "ridge_mse = mae(y_test, ridge_y_pred)\n",
        "ridge_r2 = r2(y_test, ridge_y_pred)\n",
        "\n",
        "lasso_mse = mae(y_test, lasso_y_pred)\n",
        "lasso_r2 = r2(y_test, lasso_y_pred)\n",
        "\n",
        "print(\"Ridge Regression:\")\n",
        "print(f\"Best alpha: {best_ridge_alpha}\")\n",
        "print(f\"MSE: {ridge_mse:.2f}\")\n",
        "print(f\"R-squared: {ridge_r2:.2f}\\n\")\n",
        "\n",
        "print(\"Lasso Regression:\")\n",
        "print(f\"Best alpha: {best_lasso_alpha}\")\n",
        "print(f\"MSE: {lasso_mse:.2f}\")\n",
        "print(f\"R-squared: {lasso_r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "uHfmdfWc3EOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "AAegNMq6MRN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " GridSearchCV is a technique that exhaustively searches for the best hyperparameters within a predefined set of hyperparameter values.\n",
        "\n",
        "The reason for choosing GridSearchCV is that it allows us to perform an exhaustive search over a specified range of hyperparameters. It evaluates the model's performance for each combination of hyperparameters using cross-validation and selects the best combination based on a specified scoring metric (in this case, we used negative mean squared error).\n",
        "\n",
        "By using GridSearchCV, we can ensure that we are exploring a range of hyperparameters thoroughly and selecting the best hyperparameters to build the most optimal model. GridSearchCV helps us avoid manually trying various combinations and automates the process of hyperparameter tuning, making it efficient and effective."
      ],
      "metadata": {
        "id": "aXTHWjr1MRN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Ac5Z2NSMMRN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the evaluation metrics, we can observe that the R-squared value remains the same (around 0.87) for both Ridge and Lasso Regression models, with and without hyperparameter tuning. Additionally, the Mean Squared Error is also similar, with small differences (885.89 without tuning vs. 885.87 with tuning for Lasso).\n",
        "\n",
        "In this specific case, it seems that hyperparameter tuning did not lead to a significant improvement in model performance. However, it's important to note that these models are already performing quite well with an R-squared value of around 0.87, indicating a good fit to the data."
      ],
      "metadata": {
        "id": "ec2ySfWaMRN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 -> XGBoost"
      ],
      "metadata": {
        "id": "lWVdMZiJMRN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building XGBoost Regressor Model:\n",
        "\n",
        "xgboost = xgb.XGBRegressor(objective='reg:squarederror', verbosity=0)\n",
        "xgboost.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test data using the trained model\n",
        "y_pred = xgboost.predict(X_test)"
      ],
      "metadata": {
        "id": "A_dnk3up8KKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "_DSchh4KMRN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.metrics import r2_score as r2, mean_squared_error as mse, mean_absolute_error as mae\n",
        " #Performance of the model\n",
        "r2 = r2(y_test, y_pred)\n",
        "mae = mae(y_test, y_pred)\n",
        "rmse = mse(y_test, y_pred, squared=False)\n",
        "\n",
        "# Printing the evaluation metrics\n",
        "print(\"Performance of XGBoost Regressor Model:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
        "print(f\"Root Mean Squared Error: {rmse:.2f}\")"
      ],
      "metadata": {
        "id": "9LeVLrKwMRN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "diNGAGAKMROA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score as r2, mean_squared_error as mse, mean_absolute_error as mae\n",
        "\n",
        "# Create the XGBoost regressor\n",
        "xgboost = xgb.XGBRegressor(objective='reg:linear', verbosity=0)\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "parameters = {'max_depth': [2, 5, 10],\n",
        "              'learning_rate': [0.05, 0.1, 0.2],\n",
        "              'min_child_weight': [1, 2, 5],\n",
        "              'gamma': [0, 0.1, 0.3],\n",
        "              'colsample_bytree': [0.3, 0.5, 0.7]}\n",
        "\n",
        "# RandomizedSearchCV for hyperparameter tuning with cross-validation\n",
        "xg_reg = RandomizedSearchCV(estimator=xgboost, param_distributions=parameters, n_iter=10, cv=3)\n",
        "xg_reg.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameter values and negative mean squared error\n",
        "print(\"The best parameters for XGBoost regression: \")\n",
        "for key, value in xg_reg.best_params_.items():\n",
        "    print(f\"{key}={value}\")\n",
        "print(f\"\\nNegative mean squared error: {xg_reg.best_score_}\")\n",
        "\n",
        "# Predict the test data\n",
        "y_test_pred = xg_reg.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "r2_score_test_xg = r2(y_test, y_test_pred)\n",
        "mae_test_xg = mae(y_test, y_test_pred)\n",
        "rmse_test_xg = np.sqrt(mse(y_test, y_test_pred))\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"R-squared (Test): {r2_score_test_xg:.4f}\")\n",
        "print(f\"Mean Absolute Error (Test): {mae_test_xg:.2f}\")\n",
        "print(f\"Root Mean Squared Error (Test): {rmse_test_xg:.2f}\")\n"
      ],
      "metadata": {
        "id": "1rtdGu9GjOPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "x-MYs603MROC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i have used here randomizedCV because of certain reason-\n",
        "Faster Computation: RandomizedSearchCV samples a fixed number of hyperparameter combinations randomly, whereas GridSearchCV exhaustively searches through all possible combinations. As a result, RandomizedSearchCV is generally faster because it evaluates fewer parameter settings.\n",
        "\n",
        "Flexibility: RandomizedSearchCV allows you to specify the number of iterations (n_iter) instead of specifying a specific grid of hyperparameter values. This makes it more flexible when you have a large hyperparameter space and want to explore a random subset of it.\n",
        "\n",
        "Better Exploration: RandomizedSearchCV tends to explore a broader range of hyperparameter values. This can be beneficial when the best hyperparameters are not located on the grid points of the traditional grid search.\n",
        "\n",
        "Resource-Efficient: When dealing with computationally expensive models and large datasets, RandomizedSearchCV can be more resource-efficient since it runs fewer iterations compared to GridSearchCV."
      ],
      "metadata": {
        "id": "SyvLh4RcMROE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "0mJnvh79MROF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the model's performance has improved after hyperparameter tuning, as indicated by higher R-squared and lower MAE and RMSE values. This suggests that the tuned XGBoost model is better at capturing the relationships between the features and the target variable, resulting in more accurate predictions."
      ],
      "metadata": {
        "id": "qGuhys-KMROG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "G8ex1-HyMROH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R-squared (R2 score):\n",
        "\n",
        "Indication: R-squared represents the proportion of the variance in the target variable (sales in this case) that is explained by the independent variables (features) in the model. A higher R-squared value indicates that the model explains a larger percentage of the variance, which means it fits the data well and captures the relationship between the input features and the target.\n",
        "\n",
        "Business Impact: A high R-squared value implies that the model is accurately predicting sales based on the available features. This means that the model's predictions are closer to the actual sales values, making it more reliable for making business decisions. A good R-squared score indicates that the model can provide valuable insights into the factors affecting sales, which can lead to better resource allocation, inventory management, and promotion planning, ultimately optimizing the retail business's operations and maximizing profitability.\n",
        "\n",
        "Mean Absolute Error (MAE):\n",
        "\n",
        "Indication: MAE measures the average absolute difference between the actual sales values and the predicted values by the model. A lower MAE indicates that the model's predictions are closer to the actual sales values, meaning it has lower prediction errors.\n",
        "\n",
        "Business Impact: A lower MAE signifies that the model's predictions are more accurate and closer to the actual sales figures. This accuracy is crucial for retail businesses to plan their operations and inventory management more efficiently. When the model's predictions are accurate, it helps in avoiding overstocking or understocking of products, leading to reduced inventory holding costs and improved customer satisfaction. It also aids in making more informed decisions on pricing and promotions to boost sales.\n",
        "\n",
        "Root Mean Squared Error (RMSE):\n",
        "\n",
        "Indication: RMSE measures the square root of the average of the squared differences between the actual sales values and the predicted values. Like MAE, a lower RMSE indicates that the model's predictions are closer to the actual values, but it is more sensitive to larger prediction errors.\n",
        "\n",
        "Business Impact: A lower RMSE indicates that the model is making more precise predictions, which is essential for minimizing forecasting errors in demand and sales. Minimizing RMSE helps in optimizing inventory levels, ensuring products are available when needed, and avoiding excess inventory that may lead to potential losses. Additionally, accurate predictions aid in better resource planning, cost optimization, and identifying sales opportunities during peak seasons."
      ],
      "metadata": {
        "id": "KE71nAlxMROI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3-> Decision Tree"
      ],
      "metadata": {
        "id": "X1rbwNtGMROJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# Create a Decision Tree Regressor\n",
        "decision_tree_model = DecisionTreeRegressor()\n",
        "# Fit the Algorithm\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "# Predict on the model\n",
        "y_test_pred = decision_tree_model.predict(X_test)"
      ],
      "metadata": {
        "id": "Lq2BRWASMROK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "sd4_svsTMROM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculate evaluation metrics\n",
        "r2_score_test1 = r2(y_test, y_test_pred)\n",
        "mae_test1 = mae(y_test, y_test_pred)\n",
        "rmse_test1 = mse(y_test, y_test_pred, squared=False)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Performance of Decision Tree Regressor Model:\")\n",
        "print(\"-------------------------------------------\")\n",
        "print(f\"R-squared (Test): {r2_score_test1:.4f}\")\n",
        "print(f\"Mean Absolute Error (Test): {mae_test1:.2f}\")\n",
        "print(f\"Root Mean Squared Error (Test): {rmse_test1:.2f}\")"
      ],
      "metadata": {
        "id": "oK_dk7t_MRON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "vuRUehdPMROO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Create a Decision Tree Regressor model\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [None, 5, 10, 15, 20],\n",
        "    'min_samples_split': [2, 5, 10, 15, 20],\n",
        "    'min_samples_leaf': [1, 2, 4, 8, 12],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search_cv = GridSearchCV(estimator=dt_regressor, param_grid=param_grid, cv=3)\n",
        "# Fit the Algorithm\n",
        "grid_search_cv.fit(X_train, y_train)\n",
        "# Predict on the model\n",
        "# Predict on test data\n",
        "y_test_pred_grid = grid_search_cv.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "r2_score_test_grid = r2(y_test, y_test_pred_grid)\n",
        "mae_test_grid = mae(y_test, y_test_pred_grid)\n",
        "rmse_test_grid = np.sqrt(mse(y_test, y_test_pred_grid))\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Performance of Decision Tree Regressor Model (with GridSearchCV):\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"R-squared (Test): {r2_score_test_grid:.4f}\")\n",
        "print(f\"Mean Absolute Error (Test): {mae_test_grid:.2f}\")\n",
        "print(f\"Root Mean Squared Error (Test): {rmse_test_grid:.2f}\")"
      ],
      "metadata": {
        "id": "7p8f7bWHMROP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "0uvp0BKEMROQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is used for hyperparameter tuning in this case because it performs an exhaustive search over a specified hyperparameter grid, trying all possible combinations of hyperparameters. Since Decision Tree Regressor has several hyperparameters that can significantly impact the model's performance, it is essential to find the best combination of hyperparameters for the model."
      ],
      "metadata": {
        "id": "7RwHRjtNMROR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "QfMtj6lPMROS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improvements:\n",
        "\n",
        "R-squared (Test) improved from 0.9655 to 0.9743, indicating that the model with hyperparameter tuning explains more variance in the test data, making it a better fit for the data.\n",
        "\n",
        "Mean Absolute Error (Test) reduced from 401.43 to 344.03, showing that the model with hyperparameter tuning has better accuracy in predicting the sales values, resulting in smaller absolute errors between actual and predicted sales.\n",
        "\n",
        "Root Mean Squared Error (Test) decreased from 633.21 to 546.44, indicating that the model with hyperparameter tuning has better precision in predicting the sales values, resulting in smaller errors on average.\n",
        "\n",
        "Overall, the model with GridSearchCV hyperparameter tuning outperforms the model without hyperparameter tuning in terms of predictive performance and accuracy, making it a better choice for the given dataset."
      ],
      "metadata": {
        "id": "VDbJuzENMROT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4-> Random Forest"
      ],
      "metadata": {
        "id": "Z9h2BcnqCJfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Initialize the Random Forest Regressor\n",
        "random_forest = RandomForestRegressor(random_state=42)\n",
        "# Fit the Algorithm\n",
        "random_forest.fit(X_train, y_train)\n",
        "# Predict on the model\n",
        "# Make predictions on the test data\n",
        "y_test_pred = random_forest.predict(X_test)"
      ],
      "metadata": {
        "id": "wlYGiSL2CJfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ROoqMKeGCJfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Evaluate the model's performance\n",
        "r2_score_test2 = r2(y_test, y_test_pred)\n",
        "mae_test2 = mae(y_test, y_test_pred)\n",
        "rmse_test2 = np.sqrt(mse(y_test, y_test_pred))\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Performance of Random Forest Regressor Model:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"R-squared (Test): {r2_score_test2:.4f}\")\n",
        "print(f\"Mean Absolute Error (Test): {mae_test2:.2f}\")\n",
        "print(f\"Root Mean Squared Error (Test): {rmse_test2:.2f}\")"
      ],
      "metadata": {
        "id": "-oCjF7-rCJfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "EoO1Q_hMCJfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Create a Random Forest Regressor model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_features': ['auto', 'sqrt'],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Create RandomizedSearchCV object\n",
        "randomized_search = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=5, cv=3, random_state=42)\n",
        "# Fit the Algorithm\n",
        "# Fit the model on the training data\n",
        "randomized_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best estimator from the RandomizedSearchCV\n",
        "best_params = randomized_search.best_params_\n",
        "best_rf_model = randomized_search.best_estimator_\n",
        "\n",
        "# Predict on the test data\n",
        "y_test_pred_rf = best_rf_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "r2_score_rf = r2(y_test, y_test_pred_rf)\n",
        "mae_rf = mae(y_test, y_test_pred_rf)\n",
        "rmse_rf = np.sqrt(mse(y_test, y_test_pred_rf))\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Performance of Random Forest Regressor Model (with RandomizedSearchCV):\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"R-squared (Test): {r2_score_rf:.4f}\")\n",
        "print(f\"Mean Absolute Error (Test): {mae_rf:.2f}\")\n",
        "print(f\"Root Mean Squared Error (Test): {rmse_rf:.2f}\")\n"
      ],
      "metadata": {
        "id": "UVn1QeixTGYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "bE-gLbfGCJfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Random Forest model, I used the RandomizedSearchCV hyperparameter optimization technique. The RandomizedSearchCV method randomly samples a subset of the hyperparameter space and evaluates the model's performance for each combination. This technique is more efficient and faster compared to GridSearchCV because it explores only a limited number of hyperparameter combinations, which is especially useful when dealing with large datasets or a high number of hyperparameters.\n",
        "\n",
        "The main advantage of RandomizedSearchCV is that it can efficiently find good hyperparameter configurations without exhaustively searching through all possible combinations. It allows us to specify the number of iterations (n_iter) to control the number of parameter settings that are sampled."
      ],
      "metadata": {
        "id": "j3DFnfQGCJfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "GoguBvA0CJfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, after hyperparameter tuning, the R-squared (Test) increased from 0.9784 to 0.9795, indicating a slight improvement in the model's ability to explain the variance in the test data. Additionally, the Mean Absolute Error (MAE) decreased from 322.86 to 314.08, and the Root Mean Squared Error (RMSE) decreased from 500.91 to 488.38. Both of these metrics suggest that the model's predictions are closer to the actual values, indicating improved accuracy and precision.\n",
        "\n",
        "Overall, the Random Forest model with hyperparameter tuning performed slightly better than the model without tuning, leading to a more accurate and reliable predictive performance."
      ],
      "metadata": {
        "id": "JJUkr_2uCJfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "Yugpf7urMROe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a positive business impact, we consider the following evaluation metrics for the Decision Tree model:\n",
        "\n",
        "R-squared (Coefficient of Determination): R-squared represents the proportion of variance in the dependent variable (sales) that is predictable from the independent variables (features). A higher R-squared value indicates that the model explains more variance in the sales data, which means it can better capture the underlying patterns and trends. This is important for businesses as it helps in understanding how well the model fits the data and how effective it is in predicting sales.\n",
        "\n",
        "Mean Absolute Error (MAE): MAE measures the average absolute difference between the actual sales values and the predicted sales values. A lower MAE indicates that the model has better accuracy in predicting sales, as it has smaller absolute errors between actual and predicted values. This is crucial for businesses because accurate sales predictions help in better resource allocation, inventory management, and decision-making.\n",
        "\n",
        "Root Mean Squared Error (RMSE): RMSE is the square root of the average of the squared differences between the actual sales values and the predicted sales values. It provides a measure of the precision of the model's predictions. A lower RMSE means the model's predictions are closer to the actual values on average, making it more reliable for businesses to make data-driven decisions."
      ],
      "metadata": {
        "id": "UcwVUg5MMROf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "bFm9pvzGMROh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the evaluation metric scores and the comparison of different models, the final prediction model I would choose is the Random Forest Regressor model with hyperparameter tuning using RandomizedSearchCV. This model achieved the following evaluation metric scores on the test data:\n",
        "\n",
        "**`R-squared (Test)`**: 0.9795\n",
        "\n",
        "**`Mean Absolute Error (Test)`**: 314.08\n",
        "\n",
        "**`Root Mean Squared Error (Test)`**: 488.38\n",
        "\n",
        "The Random Forest model with hyperparameter tuning outperformed the other models, including Linear Regression, XGBoost, and Decision Tree models, in terms of R-squared, MAE, and RMSE. It demonstrated the highest R-squared value, indicating a better ability to explain the variance in the test data. Additionally, it had the lowest MAE and RMSE values, suggesting higher accuracy and precision in its predictions.\n",
        "\n",
        "Furthermore, Random Forest models are known for their ability to handle non-linear relationships, interactions among features, and robustness to overfitting. By tuning the hyperparameters using RandomizedSearchCV, we were able to find the optimal combination of hyperparameters that improved the model's performance.\n",
        "\n",
        "Therefore, considering its superior performance in evaluation metrics and its robustness in handling complex data, the Random Forest Regressor model with hyperparameter tuning is chosen as the final prediction model for this dataset."
      ],
      "metadata": {
        "id": "KZbvqsKIMROh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "-sE5yHA5MROj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model used for the final prediction is the Random Forest Regressor with hyperparameter tuning using RandomizedSearchCV. Random Forest is an ensemble learning method that builds multiple decision trees during training and outputs the mean prediction of the individual trees. It is an extension of the Decision Tree algorithm and is known for its ability to handle complex data, handle non-linear relationships, and avoid overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "zaweEXCmMROj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Explainability Tool**:-\n",
        "\n",
        "One popular model explainability tool is SHAP (SHapley Additive exPlanations). SHAP values provide a unified measure of feature importance, showing the contribution of each feature to each prediction."
      ],
      "metadata": {
        "id": "WKxtSx1qjmFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap==0.42.1"
      ],
      "metadata": {
        "id": "at__98R7upWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "# Create a Tree Explainer object for the trained Random Forest model\n",
        "explainer = shap.TreeExplainer(best_rf_model)\n",
        "\n",
        "# Calculate SHAP values for the test data\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Plot the SHAP summary plot to visualize feature importance\n",
        "shap.summary_plot(shap_values, X_test)"
      ],
      "metadata": {
        "id": "orEuVLv-ltLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}