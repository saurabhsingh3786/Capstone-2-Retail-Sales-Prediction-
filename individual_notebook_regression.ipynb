{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "tEA2Xm5dHt1r",
        "fF3858GYyt-u",
        "hwyV_J3ipUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "OB4l2ZhMeS1U",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "ROoqMKeGCJfS",
        "bE-gLbfGCJfW",
        "GoguBvA0CJfY",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabhsingh3786/Capstone-2-Retail-Sales-Prediction-/blob/main/individual_notebook_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# <b><u> Project Name :- Sales Prediction : Predicting sales of a major store chain Rossmann</u></b>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Project Type**    - Regression\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 -** Saurabh Singh\n",
        "##### **Team Member 2 -** Bharathwaj Bejjarapu\n",
        "##### **Team Member 3 -** Shriya Chouhan\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.\n",
        "\n",
        "### You are provided with historical sales data for 1,115 Rossmann stores. The task is to forecast the \"Sales\" column for the test set. Note that some stores in the dataset were temporarily closed for refurbishment."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/saurabhsingh3786/Capstone-2-Retail-Sales-Prediction-\n",
        "\n"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to Develop a supervised machine learning model to accurately forecast the daily sales of Rossmann stores. Utilize historical sales data, along with additional features such as promotions, competition, holidays, seasonality, and locality, to predict the future sales for a given store. The goal is to provide reliable sales predictions that can assist store managers in making informed decisions, optimizing inventory management, and improving overall business performance.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import scipy.stats as stats\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import r2_score as r2, mean_squared_error as mse, mean_absolute_error as mae\n",
        "import math\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cj0ptgFnEnyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "path = \"/content/drive/MyDrive/AlmaBetter/Capstone Projects/Retail Sales Prediction/\"\n",
        "rossmann_sales_df = pd.read_csv(path + \"Rossmann Stores Data.csv\", low_memory=False)\n",
        "stores_df = pd.read_csv(path + \"store.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "pd.concat([rossmann_sales_df.head(),rossmann_sales_df.tail()])"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([stores_df.head(),stores_df.tail()])"
      ],
      "metadata": {
        "id": "2kfpj-skjclK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "rossmann_sales_df.shape, stores_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "rossmann_sales_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stores_df.info()"
      ],
      "metadata": {
        "id": "f1th9Tinm6pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "num_duplicates = rossmann_sales_df.duplicated().sum()\n",
        "\n",
        "# Print the result\n",
        "print(f\"The dataset has {num_duplicates} duplicate values.\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_value = stores_df[stores_df.duplicated()]\n",
        "print(\"Duplicate rows in stores dataset:\",len(duplicate_value))"
      ],
      "metadata": {
        "id": "ZOyjv0z0oJcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values\n",
        "rossmann_sales_df.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stores_df.isnull().sum()"
      ],
      "metadata": {
        "id": "b7bUbivkscCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Create a bar plot using Plotly\n",
        "missing_values = stores_df.isnull().sum().reset_index()\n",
        "missing_values.columns = ['Column', 'Missing Values']\n",
        "fig = px.bar(missing_values, x='Column', y='Missing Values')\n",
        "fig.update_layout(title='Missing Values', xaxis_title='Columns', yaxis_title='Missing Values')\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Now We try to handle missing values in stores dataset:-`**"
      ],
      "metadata": {
        "id": "gM9-JpnmvGga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of 1115 entries there are missing values for the columns:\n",
        "* CompetitionDistance- distance in meters to the nearest competitor store, the distribution plot would give us an idea about the distances at which generally the stores are opened and we would impute the values accordingly.\n",
        "\n",
        "* CompetitionOpenSinceMonth- gives the approximate month of the time the nearest competitor was opened, mode of the column would tell us the most occuring month    \n",
        "* CompetitionOpenSinceYear-  gives the approximate year of the time the nearest competitor was opened, mode of the column would tell us the most occuring month    \n",
        "* Promo2SinceWeek, Promo2SinceYear and PromoInterval are NaN wherever Promo2 is 0 or False as can be seen in the first look of the dataset. They can be replaced with 0."
      ],
      "metadata": {
        "id": "wWPz8YmmxAYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution plot of competition distance\n",
        "sns.distplot(x=stores_df['CompetitionDistance'], hist = True)\n",
        "plt.xlabel('Competition Distance Distribution Plot')"
      ],
      "metadata": {
        "id": "fwvWas4zxXbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like most of the values of the CompetitionDistance are towards the left and the distribution is skewed on the right. Median is more robust to outlier effect."
      ],
      "metadata": {
        "id": "Zix0ea8uxovG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filling competition distance with the median value\n",
        "stores_df['CompetitionDistance'].fillna(stores_df['CompetitionDistance'].median(), inplace = True)"
      ],
      "metadata": {
        "id": "pwiv0MXsxsMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filling competition open since month and year with the most occuring values of the columns i.e modes of those columns\n",
        "stores_df['CompetitionOpenSinceMonth'].fillna(stores_df['CompetitionOpenSinceMonth'].mode()[0], inplace = True)\n",
        "stores_df['CompetitionOpenSinceYear'].fillna(stores_df['CompetitionOpenSinceYear'].mode()[0], inplace = True)"
      ],
      "metadata": {
        "id": "nLjq2xASyELh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputing the nan values of promo2 related columns with 0\n",
        "stores_df['Promo2SinceWeek'].fillna(value=0,inplace=True)\n",
        "stores_df['Promo2SinceYear'].fillna(value=0,inplace=True)\n",
        "stores_df['PromoInterval'].fillna(value=0,inplace=True)"
      ],
      "metadata": {
        "id": "psR5BNKjyRMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check again for missing value if anyone left\n",
        "stores_df.isnull().sum()"
      ],
      "metadata": {
        "id": "np05uxDoyltJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rossmann Stores Data.csv - historical data including Sales\n",
        "#### store.csv  - supplemental information about the stores\n",
        "\n",
        "\n",
        "#### <u>Data fields</u>\n",
        "#### Most of the fields are self-explanatory.\n",
        "\n",
        "\n",
        "\n",
        "* **Id** - an Id that represents a (Store, Date) duple within the set\n",
        "*  **Store** - a unique Id for each store\n",
        "*  **Sales** - the turnover for any given day (Dependent Variable)\n",
        "* **Customers** - the number of customers on a given day\n",
        "* **Open** - an indicator for whether the store was open: 0 = closed, 1 = open\n",
        "* **StateHoliday** - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
        "* **SchoolHoliday** - indicates if the (Store, Date) was affected by the closure of public schools\n",
        "* **StoreType** - differentiates between 4 different store models: a, b, c, d\n",
        "* **Assortment** - describes an assortment level: a = basic, b = extra, c = extended. An assortment strategy in retailing involves the number and type of products that stores display for purchase by consumers.\n",
        "* **CompetitionDistance** - distance in meters to the nearest competitor store\n",
        "* **CompetitionOpenSince**[Month/Year] - gives the approximate year and month of the time the nearest competitor was opened\n",
        "* **Promo** - indicates whether a store is running a promo on that day\n",
        "* **Promo2** - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n",
        "* **Promo2Since**[Year/Week] - describes the year and calendar week when the store started participating in Promo2\n",
        "* **PromoInterval** - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(list(rossmann_sales_df.columns))\n",
        "print(list(stores_df.columns))"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "rossmann_sales_df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stores_df.describe().T"
      ],
      "metadata": {
        "id": "6w6Z0JJq7meX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rossmann_sales_df.dtypes"
      ],
      "metadata": {
        "id": "c-rITgIn7-gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change datatype of date column\n",
        "# code for changing format of date from object to datetime\n",
        "rossmann_sales_df['Date'] = pd.to_datetime(rossmann_sales_df['Date'], format= '%Y-%m-%d')"
      ],
      "metadata": {
        "id": "9PIdbV9q8l8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change datatype of stateholiday\n",
        "rossmann_sales_df['StateHoliday'].unique()"
      ],
      "metadata": {
        "id": "1w_6hPeA9tlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rossmann_sales_df['StateHoliday'] = rossmann_sales_df['StateHoliday'].replace({'a': 1, 'b': 2, 'c': 3, '0': 0})"
      ],
      "metadata": {
        "id": "Ue78TNulD9Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we check datatypes of stores dataset and check if there have to any correction."
      ],
      "metadata": {
        "id": "Y6ONeKqjEOv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stores_df.dtypes"
      ],
      "metadata": {
        "id": "oqXmGcjhEfZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change datatype of Assortment and storetype\n",
        "stores_df['Assortment'] = stores_df['Assortment'].replace({'a': 0, 'b': 1, 'c': 2})\n",
        "stores_df['StoreType'] = stores_df['StoreType'].replace({'a': 0, 'b': 1, 'c': 2,'d': 3})"
      ],
      "metadata": {
        "id": "R_dyAF5qKoVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stores_df['CompetitionDistance']= stores_df['CompetitionDistance'].astype(int)\n",
        "stores_df['CompetitionOpenSinceMonth'] = stores_df['CompetitionOpenSinceMonth'].astype(int)\n",
        "stores_df['CompetitionOpenSinceYear']= stores_df['CompetitionOpenSinceYear'].astype(int)\n",
        "stores_df['Promo2SinceWeek']= stores_df['Promo2SinceWeek'].astype(int)\n",
        "stores_df['Promo2SinceYear']= stores_df['Promo2SinceYear'].astype(int)"
      ],
      "metadata": {
        "id": "rQScWZn4NoPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "#unique variables in rossmann sales dataset.\n",
        "print(rossmann_sales_df.apply(lambda col: col.unique()))"
      ],
      "metadata": {
        "id": "G_rK6lNaKudE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unique variables in stores dataset.\n",
        "print(stores_df.apply(lambda col: col.unique()))"
      ],
      "metadata": {
        "id": "RFObuUyCDJUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "#now we merge both dataset to make analysis.\n",
        "merged_df = pd.merge(rossmann_sales_df, stores_df, on='Store', how='left')"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.head()"
      ],
      "metadata": {
        "id": "HmKxoKS6BDov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`UNIVARIATE ANALYSIS`:-"
      ],
      "metadata": {
        "id": "oWAhXVLcGq_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question-1: what is sales distribution across stores.\n",
        "# Group the sales by store and calculate the total sales\n",
        "store_sales = merged_df.groupby('Store')['Sales'].sum().reset_index()\n",
        "store_sales"
      ],
      "metadata": {
        "id": "Td3ey0dpGxgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#question-2: number of stores in each store type?\n",
        "# Data Wrangling for Store-Related Features\n",
        "store_features = merged_df[['Store', 'StoreType']]\n",
        "\n",
        "# Count the number of stores in each store type\n",
        "store_counts = store_features['StoreType'].value_counts()\n",
        "store_counts"
      ],
      "metadata": {
        "id": "z6lLDIjaKI5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question-3: how are sales affected by promotional activities?\n",
        "# Data Wrangling for Sales and Promo Columns\n",
        "sales_promo_data = merged_df[['Sales', 'Promo', 'Date']]\n",
        "\n",
        "# Convert the 'Date' column to datetime data type\n",
        "sales_promo_data['Date'] = pd.to_datetime(sales_promo_data['Date'])\n",
        "\n",
        "# Group the data by Promo (0: Non-Promo, 1: Promo) and calculate average sales for each group\n",
        "promo_grouped = sales_promo_data.groupby('Promo')['Sales'].mean()\n",
        "promo_grouped"
      ],
      "metadata": {
        "id": "vqu9FqGXP531"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`BIVARIATE ANALYSIS`"
      ],
      "metadata": {
        "id": "ndtK-hH9U_Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question-4: How does the average sales vary with respect to different store types?\n",
        "# Data Wrangling for Store Type vs. Sales\n",
        "store_sales_data = merged_df[['Store', 'StoreType', 'Sales']]\n",
        "\n",
        "# Calculate average sales and standard deviation for each store type\n",
        "avg_sales_by_store_type = store_sales_data.groupby('StoreType')['Sales'].mean()\n",
        "std_sales_by_store_type = store_sales_data.groupby('StoreType')['Sales'].std()\n",
        "print(\"average sales by storetype is:\", avg_sales_by_store_type)"
      ],
      "metadata": {
        "id": "RhvH6XQTU8-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question-5: What is the correlation between sales and the competition distance?\n",
        "\n",
        "# Data Wrangling for Sales vs. Competition Distance\n",
        "sales_competition_data = merged_df[['Sales', 'CompetitionDistance']]\n",
        "\n",
        "print(\"sales competiton data:\",sales_competition_data)\n",
        "\n",
        "# Calculate the correlation coefficient between Sales and CompetitionDistance\n",
        "correlation_coefficient = sales_competition_data['Sales'].corr(sales_competition_data['CompetitionDistance'])\n",
        "print(correlation_coefficient)"
      ],
      "metadata": {
        "id": "paZM9Oo_kKxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question-6: How does sales vary across different days of the week?\n",
        "# Data Wrangling for Sales vs. DayOfWeek\n",
        "sales_day_data = merged_df[['Sales', 'DayOfWeek']]\n",
        "\n",
        "# Replace missing values (NaN) in the 'DayOfWeek' column with 'Sunday'\n",
        "sales_day_data['DayOfWeek'].fillna('Sunday', inplace=True)\n",
        "\n",
        "# Group the data by DayOfWeek and calculate average sales for each day\n",
        "average_sales_by_day = sales_day_data.groupby('DayOfWeek')['Sales'].mean().reset_index()\n",
        "\n",
        "# Rename the DayOfWeek values to meaningful day names\n",
        "day_names = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
        "average_sales_by_day['DayOfWeek'] = average_sales_by_day['DayOfWeek'].map(dict(enumerate(day_names)))\n",
        "\n",
        "# Sort the data by DayOfWeek for proper visualization\n",
        "average_sales_by_day['DayOfWeek'] = pd.Categorical(average_sales_by_day['DayOfWeek'], categories=day_names, ordered=True)\n",
        "average_sales_by_day.sort_values('DayOfWeek', inplace=True)\n",
        "print(average_sales_by_day)\n",
        "\n"
      ],
      "metadata": {
        "id": "sMI3wg4zrh3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question-7: What is the effect of school holidays on sales?\n",
        "# Data Wrangling for Sales vs. School Holidays\n",
        "sales_school_holidays_data = merged_df[['Sales', 'SchoolHoliday']]\n",
        "\n",
        "# Group the data by SchoolHoliday and calculate average sales for each category\n",
        "average_sales_by_school_holiday = sales_school_holidays_data.groupby('SchoolHoliday')['Sales'].mean().reset_index()\n",
        "\n",
        "# Replace 0 and 1 in the 'SchoolHoliday' column with meaningful labels for visualization\n",
        "average_sales_by_school_holiday['SchoolHoliday'] = average_sales_by_school_holiday['SchoolHoliday'].map({0: 'No Holiday', 1: 'School Holiday'})\n",
        "average_sales_by_school_holiday\n",
        "\n"
      ],
      "metadata": {
        "id": "LCgTaIkIupSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`MULTIVARIATE ANALYSIS`"
      ],
      "metadata": {
        "id": "wx1LScDC0GP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question-8: is there any correlation between features?\n",
        "# Selecting only numerical variables for the correlation matrix\n",
        "numerical_vars = merged_df.select_dtypes(include='number')\n",
        "\n",
        "# Handling missing data (if any)\n",
        "numerical_vars = numerical_vars.dropna()\n"
      ],
      "metadata": {
        "id": "vlyg8ECj0NHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question-9: relation between promo vs sales vs customers?\n",
        "\n",
        "\n",
        "# Selecting the relevant columns for analysis\n",
        "promo_sales_customers_data = merged_df[['Promo', 'Sales', 'Customers']]\n",
        "\n",
        "# Handling missing data (if any)\n",
        "promo_sales_customers_data = promo_sales_customers_data.dropna()\n",
        "promo_sales_customers_data"
      ],
      "metadata": {
        "id": "uuc8x9jjmkGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "after doing EDA process we get to know about sales distribution across store like how the sales are going on among stores, then how many numbers of stores are there in each store type and is sales are affected by any promotional activities as sales are increasing or decreasing when there is any promotional activity, after that we calculate average sales with respect to different store type to check how sales are varying across store type. then we checked correlation between sales and competitiondistance to check if there any effect of distance on sales. After that, we get to know about which day of week is getting highest sales and effect of school holiday on sales like does it impact positively on sales or not and in the end, we just finally check relationship between sales and customer when there is promo present and absent.   "
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "#Question-1: what is sales distribution across stores.\n",
        "\n",
        "\n",
        "# Increase figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the distribution of sales with customized colors and grid lines\n",
        "sns.distplot(merged_df['Sales'], color='skyblue', kde_kws={'color': 'darkblue', 'linewidth': 2})\n",
        "plt.xlabel('Sales')\n",
        "plt.ylabel('Density')\n",
        "plt.title(\"Distribution of Sales\")\n",
        "\n",
        "# Add grid lines\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Remove top and right spines\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The primary purpose of this chart is to visualize the distribution of the 'Sales' data. Histograms provide a clear representation of the data distribution by dividing the data into bins and displaying the frequency of values falling into each bin. The KDE plot complements the histogram by providing a smooth estimate of the probability density function of the data, helping to reveal the underlying distribution."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Central Tendency: The chart can reveal the central tendency of the sales data, indicating the typical or average level of sales. This is often represented by the peak(s) or modes in the distribution.\n",
        "\n",
        "Spread: The chart can provide insights into the spread or variability of sales across different values. Wider distributions indicate higher variability, while narrower distributions suggest relatively consistent sales levels.\n",
        "\n",
        "Skewness: The skewness of the distribution can be observed from the histogram and KDE plot. Positive skewness indicates a longer right tail, suggesting that a few stores have exceptionally high sales. Negative skewness implies a longer left tail, indicating a few stores with very low sales."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying High-Performing Stores: Insights about stores with consistently high sales and positive skewness can help identify top-performing stores. These insights can lead to allocating more resources, marketing efforts, and promotions to further boost sales in these stores.\n",
        "\n",
        "Targeting Underperforming Stores: Understanding the sales distribution can help identify stores with low sales or negative skewness (long left tail). These insights can lead to targeted interventions, such as improving store operations, introducing new products, or implementing better marketing strategies to revitalize underperforming stores."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "#question-2: number of stores in each store type?\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=store_features, x='StoreType', palette='pastel')\n",
        "plt.xlabel('Store Type')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Number of Stores in Each Store Type')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sns.countplot() function from the Seaborn library is specifically designed to show the count of occurrences of categorical data in a dataset. It is a specialized bar plot that displays the frequency of unique values in a categorical variable."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of stores are highest in type 0 store."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "Identifying High-Performing Stores: Identifying stores with consistently high sales and positive performance metrics can help focus resources on these stores to further boost sales and profitability. Targeted marketing campaigns and investments can lead to increased revenue.\n",
        "\n",
        "Understanding Customer Behavior: Analyzing customer behavior patterns, such as buying preferences and peak shopping times, can enable businesses to tailor their offerings and promotions to better meet customer needs. Improved customer satisfaction can lead to increased customer loyalty and positive word-of-mouth.\n",
        "\n",
        "Optimizing Inventory and Supply Chain: Insights into sales patterns and demand fluctuations can aid in inventory management. Maintaining optimal stock levels can prevent stockouts, reduce holding costs, and enhance operational efficiency.\n",
        "\n",
        "\n",
        "Negative Business Impact:\n",
        "\n",
        "Ignoring Underperforming Stores: Failure to identify and address underperforming stores can lead to revenue loss and decreased profitability. Neglecting these stores may result in missed opportunities to improve their performance.\n",
        "\n",
        "Misinterpreting Customer Behavior: Misinterpreting customer behavior insights might lead to misguided marketing efforts or product offerings. This could lead to reduced customer satisfaction and a negative impact on sales.\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "#Question-3: how are sales affected by promotional activities?\n",
        "\n",
        "# Create a line plot to compare sales for promotional and non-promotional days\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=sales_promo_data, x='Date', y='Sales', hue='Promo', palette='Set1')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.title('Average Sales during Promotional and Non-Promotional Days')\n",
        "plt.legend(title='Promo', labels=['Non-Promo', 'Promo'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the line plot is a suitable choice for visualizing the impact of promotional activities on sales because of its ability to present time-series data, facilitate comparison, and highlight trends over time. It provides a clear and insightful representation of how promotional activities influence sales, making it an effective chart for communicating insights to business stakeholders and decision-makers."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average sales were highest for both promo and non promo in period of 2013-09 to 2014-01."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous monitoring and evaluation of promotional activities, along with periodic refinements based on performance data, are essential for sustained positive growth. Data-driven decision-making and a customer-centric approach are key to leveraging insights for positive business impact."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Chart-4 Visualization Code\n",
        "#Question-4: How does the average sales vary with respect to different store types?\n",
        "\n",
        "# Create a bar plot with error bars to compare average sales for different store types\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(data=store_sales_data, x='StoreType', y='Sales', ci='sd')\n",
        "plt.errorbar(x=avg_sales_by_store_type.index, y=avg_sales_by_store_type, yerr=std_sales_by_store_type, fmt='none', color='black', capsize=5)\n",
        "plt.xlabel('Store Type')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.title('Average Sales Variation by Store Type')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nnXp5rhmWypx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar plot with error bars is used here to visualize the average sales for different store types while also representing the uncertainty or variability in these average sales values. This is particularly useful when dealing with aggregated data, such as average sales by store type, where we want to show not only the central tendency (average) but also the spread or dispersion of the data."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average sales by store type 1 is highest among all."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure a positive business impact, it is crucial to interpret the insights in the context of the overall business strategy and goals. Businesses should use the insights to inform their decisions, implement targeted strategies, and continuously evaluate performance."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "#Question-5: What is the correlation between sales and the competition distance?\n",
        "# Create a scatter plot to visualize the relationship between Sales and CompetitionDistance\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(data=sales_competition_data, x='CompetitionDistance', y='Sales')\n",
        "plt.xlabel('Competition Distance')\n",
        "plt.ylabel('Sales')\n",
        "plt.title(f'Correlation between Sales and Competition Distance\\nCorrelation Coefficient: {correlation_coefficient:.2f}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot allows us to visualize the pattern of data points and identify any potential correlation between sales and competition distance. The correlation coefficient gives us a numerical measure of the strength and direction of the relationship. A positive correlation coefficient indicates a positive relationship, while a negative coefficient indicates a negative relationship. A value close to 0 suggests a weak or no correlation."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a correlation coefficient value of -0.0189 indicates a weak or negligible linear relationship between sales and competition distance in the dataset. For business decision-making, it suggests that competition distance alone may not be a strong predictor of sales performance, and other factors might have a more significant impact on sales in the retail stores."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the weak correlation between sales and competition distance highlights the importance of considering a holistic approach to sales performance improvement. While competition distance is one of the many factors that can influence sales, relying solely on this factor might not lead to significant business impact. Instead, businesses should adopt a comprehensive strategy, focusing on customer preferences, competitive advantage, and store-specific factors to drive positive growth."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "#Question-6: How does sales vary across different days of the week?\n",
        "\n",
        "# Create a bar plot to compare average sales across different days of the week\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(data=average_sales_by_day, x='DayOfWeek', y='Sales')\n",
        "plt.xlabel('Day of Week')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.title('Average Sales Variation Across Different Days of the Week')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the bar plot is a suitable choice for visualizing the average sales variation across different days of the week due to its ability to represent categorical data and facilitate easy comparisons between the days. It allows us to observe the distribution of sales over days and identify patterns or trends that may inform business decisions related to marketing strategies, promotions, or staffing based on the sales performance on specific days of the week."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "when there is sunday sales are very low and on monday sales are highest."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimized Resource Allocation: Knowing that Sunday has the lowest sales, businesses can adjust their resource allocation accordingly. For example, they may reduce staffing levels or operational expenses on Sundays when the demand is lower, leading to cost savings.\n",
        "\n",
        "Targeted Marketing Strategies: Understanding the sales variation across different days of the week can inform targeted marketing strategies. Businesses can focus their promotions and advertising efforts on other days with higher sales potential, such as weekdays or Saturdays, to capitalize on peak demand.\n",
        "\n",
        "Enhanced Promotional Planning: Businesses can plan promotions and discounts strategically to boost sales on Sundays and attract more customers during traditionally slower periods. Special Sunday-only offers or exclusive deals could entice customers to visit the store on Sundays."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "#Question-7: What is the effect of school holidays on sales?\n",
        "# Create a bar plot to compare average sales during school holidays and non-school holidays\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.barplot(data=average_sales_by_school_holiday, x='SchoolHoliday', y='Sales')\n",
        "plt.xlabel('School Holiday')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.title('Effect of School Holidays on Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'SchoolHoliday' column is a categorical variable with two categories: 0 (Non-School Holiday) and 1 (School Holiday). Bar plots are commonly used to represent and compare data in categories, making them suitable for visualizing average sales for each category."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "when its school holiday sales are increased a bit."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimized Promotional Strategies: With higher sales during school holidays, businesses can focus on optimizing their promotional strategies during these periods. They can plan targeted promotions and special offers to attract more customers and maximize sales during school holiday seasons.\n",
        "\n",
        "Staffing and Inventory Management: The increased sales during school holidays may necessitate adjustments in staffing and inventory management. Businesses can schedule more staff during peak periods to handle higher customer demands and ensure sufficient stock availability to meet increased sales.\n",
        "\n",
        "Marketing and Advertising: The insight regarding higher sales during school holidays can guide marketing and advertising efforts. Businesses can allocate more resources to advertising campaigns specifically designed for school holiday shoppers, reaching out to the right audience and driving higher footfall."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "#Question-8: is there any correlation between features?\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = numerical_vars.corr()\n",
        "plt.figure(figsize=(18,8))\n",
        "correlation = merged_df.corr()\n",
        "sns.heatmap(abs(correlation), annot=True, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " the heatmap, was selected because it effectively represents the correlation matrix and allows us to visually identify patterns and relationships between numerical variables in the merged_df DataFrame."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "most of the features have negative correlation between them as showed by heatmap."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact: Insights that reveal positive correlations or relationships between certain variables and sales can have a positive business impact. For example, if the analysis shows that promotions positively impact sales, the business can focus on running more effective promotional campaigns to increase revenue. Similarly, if the correlation heatmap highlights that specific product assortments lead to higher sales, the business can adjust its inventory strategy accordingly.\n",
        "\n",
        "Negative Growth Considerations: On the other hand, insights indicating negative correlations or relationships can highlight areas where improvements are needed. For instance, if there is a negative correlation between sales and competition distance, it may suggest that stores located farther from competitors might face challenges in attracting customers. In such cases, the business might consider strategies like targeted marketing, customer loyalty programs, or store location optimization."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "#Question-9: relation between promo vs sales vs customers?\n",
        "\n",
        "# Create a scatter plot to visualize the relationship between 'Promo', 'Sales', and 'Customers'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=promo_sales_customers_data, x='Sales', y='Customers', hue='Promo', palette='coolwarm', alpha=0.7)\n",
        "plt.xlabel('Sales')\n",
        "plt.ylabel('Customers')\n",
        "plt.title('Promo vs. Sales vs. Customers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a scatter plot to visualize the relationship between \"Promo,\" \"Sales,\" and \"Customers\" because it is suitable for analyzing the correlation between two numerical variables (in this case, \"Sales\" and \"Customers\") and how it is influenced by a categorical variable (in this case, \"Promo\")."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sales are increasing as there are more customers when promo is 1."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the scatter plot shows a clear positive correlation between promotions, sales, and customers, it may indicate that promotions lead to increased sales and higher customer footfall. In such cases, businesses can leverage this insight by strategically planning and optimizing promotions to boost revenue and customer engagement. For instance, offering attractive discounts or special deals during promotions might encourage more customers to make purchases, resulting in positive growth for the business."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on our EDA work we will now give three hypothesis statement and perform p test and give final conclusion based on our hypothesis."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0)**: There is no correlation between the presence of promotions (Promo) and the number of customers (Customers) visiting the stores.\n",
        "\n",
        "**Alternate Hypothesis (H1)**: There is a positive correlation between the presence of promotions (Promo) and the number of customers (Customers) visiting the stores."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Statement 1: Correlation between 'Promo' and 'Customers'\n",
        "corr_promo_customers, p_value_1 = stats.pearsonr(merged_df['Promo'], merged_df['Customers'])\n",
        "\n",
        "# Print the correlation coefficients and p-values for each statement\n",
        "print(\"Statement 1: Correlation between Promo and Customers\")\n",
        "print(f\"Correlation coefficient: {corr_promo_customers:.4f}, p-value: {p_value_1:.4f}\\n\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation coefficient (0.3162) indicates a positive correlation between the presence of promotions (Promo) and the number of customers (Customers) visiting the stores. A positive correlation coefficient means that as the presence of promotions increases, the number of customers visiting the stores also tends to increase.\n",
        "\n",
        "The p-value (0.0000) is extremely small, indicating that the correlation between \"Promo\" and \"Customers\" is statistically significant."
      ],
      "metadata": {
        "id": "U3cDmig14JJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code provided earlier to test the correlation between \"Promo\" and \"Customers,\" I used the Pearson correlation coefficient and the `stats.pearsonr()` function from the `scipy.stats` module to obtain the p-value."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Pearson correlation coefficient as the specific statistical test to analyze the relationship between \"Promo\" and \"Customers\" because both variables are continuous numerical variables, and we want to understand the linear correlation between them.\n",
        "\n",
        "The Pearson correlation coefficient is a widely used measure to quantify the strength and direction of a linear relationship between two continuous variables."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0)**: There is no correlation between the presence of promotions (Promo) and the total sales (Sales) generated by the stores.\n",
        "\n",
        "**Alternate Hypothesis (H1)**: There is a positive correlation between the presence of promotions (Promo) and the total sales (Sales) generated by the stores."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Statement 2: Correlation between 'Promo' and 'Sales'\n",
        "corr_promo_sales, p_value_2 = stats.pearsonr(merged_df['Promo'], merged_df['Sales'])\n",
        "print(\"Statement 2: Correlation between Promo and Sales\")\n",
        "print(f\"Correlation coefficient: {corr_promo_sales:.4f}, p-value: {p_value_2:.4f}\\n\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation Coefficient: The correlation coefficient (0.4523) indicates a moderate positive correlation between the presence of promotions (Promo) and the total sales (Sales) generated by the stores. A positive correlation coefficient suggests that as the presence of promotions increases, the total sales tend to increase as well.\n",
        "\n",
        "p-value: The p-value obtained from the statistical test is very small (p-value: 0.0000), which indicates that the correlation between \"Promo\" and \"Sales\" is statistically significant."
      ],
      "metadata": {
        "id": "JE6cNZWu7YXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code provided earlier to test the correlation between \"Promo\" and \"Customers,\" I used the Pearson correlation coefficient and the `stats.pearsonr()` function from the `scipy.stats` module to obtain the p-value."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Pearson correlation coefficient as the specific statistical test to analyze the relationship between \"Promo\" and \"Customers\" because both variables are continuous numerical variables, and we want to understand the linear correlation between them.\n",
        "\n",
        "The Pearson correlation coefficient is a widely used measure to quantify the strength and direction of a linear relationship between two continuous variables."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0)**: There is no correlation between the distance to the nearest competitor (CompetitionDistance) and the total sales (Sales) generated by the stores.\n",
        "\n",
        "**Alternate Hypothesis (H1)**: There is a negative correlation between the distance to the nearest competitor (CompetitionDistance) and the total sales (Sales) generated by the stores."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Statement 3: Correlation between 'CompetitionDistance' and 'Sales'\n",
        "corr_competition_distance_sales, p_value_3 = stats.pearsonr(merged_df['CompetitionDistance'], merged_df['Sales'])\n",
        "\n",
        "print(\"Statement 3: Correlation between CompetitionDistance and Sales\")\n",
        "print(f\"Correlation coefficient: {corr_competition_distance_sales:.4f}, p-value: {p_value_3:.4f}\\n\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation Coefficient: The correlation coefficient (-0.0189) is very close to zero and negative. This indicates a weak and negative correlation between the distance to the nearest competitor (CompetitionDistance) and the total sales (Sales) generated by the stores. A negative correlation coefficient suggests that as the distance to the nearest competitor increases, the total sales may slightly decrease, but the correlation is very weak.\n",
        "\n",
        "p-value: The p-value obtained from the statistical test is very small (p-value: 0.0000), which indicates that the correlation between \"CompetitionDistance\" and \"Sales\" is statistically significant."
      ],
      "metadata": {
        "id": "2xpJFgee9ZRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value\n",
        "# Check for missing values in each column of the merged dataset\n",
        "missing_values_count = merged_df.isnull().sum()\n",
        "print(missing_values_count)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see there is no missing value left in this merged dataset because we already treat them before doing EDA process."
      ],
      "metadata": {
        "id": "-aNxxy5U5ZM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used median and mode for missing value imputation.\n",
        "\n",
        "`Median Imputation`:- It Replace missing values with the median (for skewed data) of the available values in the column.I Used This method Because It is suitable for numerical data with no significant outliers. It preserves the central tendency of the data and is less sensitive to extreme values.\n",
        "\n",
        "`Mode Imputation`:- It Replace missing values with the mode (most frequent value) of the available values in the column (for categorical data). I use it Because Mode imputation works well for categorical data with a low number of unique categories. It preserves the most common category distribution."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Select only numerical columns for box plot visualization\n",
        "numerical_cols = merged_df.select_dtypes(include='number').columns\n",
        "\n",
        "# Create box plots for numerical columns to visualize potential outliers\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(data=merged_df[numerical_cols])\n",
        "plt.title(\"Box Plot for Numerical Columns (to identify outliers)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we will calculate percentage of outliers in each features after that we will handle outlier in respective features."
      ],
      "metadata": {
        "id": "kFBMuF9Skkq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Identify numerical columns with potential outliers\n",
        "numerical_cols = merged_df.select_dtypes(include='number').columns\n",
        "\n",
        "# Set the z-score threshold for identifying outliers\n",
        "z_score_threshold = 3\n",
        "\n",
        "# Dictionary to store the percentage of outliers for each numerical column\n",
        "percentage_of_outliers = {}\n",
        "\n",
        "# Loop through each numerical column and calculate the percentage of outliers\n",
        "for col in numerical_cols:\n",
        "    col_mean = merged_df[col].mean()\n",
        "    col_std = merged_df[col].std()\n",
        "    z_scores = np.abs((merged_df[col] - col_mean) / col_std)\n",
        "    num_outliers = len(merged_df[z_scores > z_score_threshold])\n",
        "    percentage = (num_outliers / len(merged_df)) * 100\n",
        "    percentage_of_outliers[col] = percentage\n",
        "\n",
        "# Print the percentage of outliers for each numerical column\n",
        "for col, percentage in percentage_of_outliers.items():\n",
        "    print(f\"Percentage of outliers in {col}: {percentage:.2f}%\")\n"
      ],
      "metadata": {
        "id": "7X2dZSLClYB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing outliers From Sales, Customers, StateHoliday, CompetitionDistance And CompetitionOpenSinceYear-"
      ],
      "metadata": {
        "id": "-NazljKtmDzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the features for which outliers will be removed\n",
        "features_with_outliers = ['Sales', 'Customers', 'StateHoliday', 'CompetitionDistance', 'CompetitionOpenSinceYear']\n",
        "\n",
        "# Set the z-score threshold for identifying outliers\n",
        "z_score_threshold = 3\n",
        "\n",
        "# Loop through each feature and remove rows with outliers based on z-scores\n",
        "for feature in features_with_outliers:\n",
        "    feature_mean = merged_df[feature].mean()\n",
        "    feature_std = merged_df[feature].std()\n",
        "    z_scores = np.abs((merged_df[feature] - feature_mean) / feature_std)\n",
        "    merged_df = merged_df[z_scores <= z_score_threshold]\n"
      ],
      "metadata": {
        "id": "I_lY6DfkndP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we check again if any outlier remaining in dataset."
      ],
      "metadata": {
        "id": "czJ0gAcooYfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify numerical columns with potential outliers\n",
        "numerical_cols = merged_df.select_dtypes(include='number').columns\n",
        "\n",
        "# Set the z-score threshold for identifying outliers\n",
        "z_score_threshold = 3\n",
        "\n",
        "# Check for outliers in each numerical column\n",
        "outliers_exist = False\n",
        "for col in numerical_cols:\n",
        "    col_mean = merged_df[col].mean()\n",
        "    col_std = merged_df[col].std()\n",
        "    z_scores = np.abs((merged_df[col] - col_mean) / col_std)\n",
        "    if any(z_scores > z_score_threshold):\n",
        "        outliers_exist = True\n",
        "        print(f\"Outliers still exist in column '{col}'\")\n",
        "\n",
        "if not outliers_exist:\n",
        "    print(\"No outliers remaining in the dataset.\")\n"
      ],
      "metadata": {
        "id": "SXbwLh5VoPZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we used the z-score method for outlier treatment. Z-score is a widely used technique to identify and handle outliers in a dataset. It helps us understand how far each data point is from the mean in terms of standard deviations.\n",
        "\n",
        "Here's why we used the z-score method for outlier treatment:\n",
        "\n",
        "**1. Z-Score Method**: The z-score method is suitable when dealing with data that is approximately normally distributed. It helps to identify extreme values (outliers) that are significantly different from the rest of the data in terms of standard deviations. By setting a threshold (e.g., z-score threshold of 3), we can determine which data points are outliers and subsequently handle them based on the analysis goals.\n",
        "\n",
        "**2. Applicability to Numerical Data**: The z-score method is well-suited for numerical data as it requires computing the mean and standard deviation, which are meaningful statistical measures for numerical features.\n",
        "\n",
        "**3. Robustness to Scale**: The z-score method is robust to the scale of the data, as it standardizes each data point based on the mean and standard deviation. This allows us to compare and identify outliers in features with different units and scales.\n",
        "\n",
        "**4. Easy Implementation**: The z-score method is straightforward to implement and is readily available in most data analysis libraries, such as NumPy and Pandas."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Assuming 'merged_df' is the merged dataset\n",
        "categorical_columns = merged_df.select_dtypes(include='object').columns\n",
        "\n",
        "# Print the list of categorical columns\n",
        "print(\"Categorical Columns:\")\n",
        "print(categorical_columns)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Perform one-hot encoding for 'PromoInterval'\n",
        "merged_df = pd.get_dummies(merged_df, columns=['PromoInterval'], prefix='PromoInterval', drop_first=True)\n",
        "\n",
        "# Now, the 'PromoInterval' feature has been one-hot encoded into binary columns.\n"
      ],
      "metadata": {
        "id": "cZVhPDZMA5mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "as there is only one feature here with object datatype so we dont need any type of encoding here because we will not use this feature in regression as this have not so much importance."
      ],
      "metadata": {
        "id": "PmXy1s9gv8Nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One-Hot Encoding**: One-hot encoding is a technique used to convert categorical variables with multiple categories into binary vectors. It creates binary columns for each unique category, representing whether the category is present (1) or not (0). We used one-hot encoding for the 'PromoInterval' feature because it is a nominal categorical variable with multiple non-ordinal categories (e.g., Jan, Feb, Mar).\n",
        "\n",
        "**Reason for using one-hot encoding**: One-hot encoding is suitable for nominal categorical variables like 'PromoInterval' because it doesn't introduce any ordinal relationship between the categories and avoids biasing the model towards any particular category. Additionally, one-hot encoding allows machine learning models to easily understand and interpret categorical features."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the information provided about the dataset, there doesn't seem to be any textual data present in the dataset. The dataset primarily contains numerical and categorical features related to store information, sales, promotions, and other relevant factors. Textual data preprocessing is typically required when dealing with natural language processing (NLP) tasks or text-based data, which is not the case in this dataset."
      ],
      "metadata": {
        "id": "BV66wxR0xYld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets remove store since we need sales of all stores, not a particular one and also sales can be predicted through store type, assortment, etc."
      ],
      "metadata": {
        "id": "CfPoDsUW1FI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# drop Store\n",
        "merged_df.drop('Store', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N6K4Ml0cIRr"
      },
      "source": [
        "Lets remove date since there are already day of week and week of year features in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXyxq--ecIRr"
      },
      "outputs": [],
      "source": [
        "# drop Date\n",
        "merged_df.drop('Date', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lG34OGPcbLw"
      },
      "source": [
        "Lets remove competition open since month and competition open since year as the information provided by them can be obtained from competition open number of months."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfbyPuHfd36q"
      },
      "outputs": [],
      "source": [
        "# drop CompetitionOpenSinceMonth & CompetitionOpenSinceYear\n",
        "merged_df.drop(['CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6pQLzqMdqZP"
      },
      "source": [
        "Lets remove promo 2, promo 2 since week and promo 2 since year as the information provided by them can be obtained from promo 2 number of weeks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kw9snCWmeI30"
      },
      "outputs": [],
      "source": [
        "# drop Promo2, Promo2SinceWeek & Promo2SinceYear\n",
        "merged_df.drop(['Promo2', 'Promo2SinceWeek', 'Promo2SinceYear'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#no of observations for closed stores with 0 sales\n",
        "(merged_df[merged_df.Open == 0]).shape"
      ],
      "metadata": {
        "id": "jPV6_zrL2z4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#since the stores closed had 0 sale value; removing the irrelevant part\n",
        "meerged_df = merged_df[merged_df.Open != 0]\n",
        "merged_df.drop('Open', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "_j55xqcY3Dt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ES3CwZTsn5f"
      },
      "outputs": [],
      "source": [
        "# exploring the head of the resultant dataframe\n",
        "merged_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "# Separate the feature matrix 'X' and the target variable 'y'\n",
        "X = merged_df.drop(columns=['Sales'])\n",
        "y = merged_df['Sales']\n",
        "\n",
        "# Number of top features to select\n",
        "k = 10\n",
        "\n",
        "# Perform feature selection using ANOVA\n",
        "selector = SelectKBest(score_func=f_regression, k=k)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Get the indices of the selected features\n",
        "selected_feature_indices = selector.get_support(indices=True)\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_feature_names = X.columns[selected_feature_indices]\n",
        "selected_feature_scores = selector.scores_[selected_feature_indices]\n",
        "# Now, 'X_selected' contains only the selected features, and 'selected_feature_names' contains their names.\n",
        "# Print the selected features and their corresponding ANOVA F-values\n",
        "print(\"Selected Features:\")\n",
        "for feature, score in zip(selected_feature_names, selected_feature_scores):\n",
        "    print(f\"{feature}: ANOVA F-value = {score}\")"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code for feature selection using SelectKBest with ANOVA, we selected the top 'k' features based on their statistical significance in relation to the target variable 'Sales'. The higher the ANOVA F-value for a feature, the more important it is in explaining the variation in the target variable.\n",
        "\n",
        "After running the feature selection code, we can access the selected features and their corresponding ANOVA F-values using the scores_ attribute of the SelectKBest object."
      ],
      "metadata": {
        "id": "Ia02jJY8CjjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code for feature selection in the merged dataset, we used the SelectKBest method with the ANOVA (Analysis of Variance) score function. Let's discuss the feature selection methods used and the reasons for choosing them:\n",
        "\n",
        "SelectKBest with ANOVA: SelectKBest is a feature selection method from scikit-learn that selects the top 'k' features based on univariate statistical tests. The ANOVA score function is used specifically for regression tasks (predicting continuous target variables) and evaluates the relationship between each feature and the target variable using ANOVA F-values.\n",
        "\n",
        "Reason for using SelectKBest with ANOVA: We chose this method because the target variable 'Sales' is a continuous numerical variable in the regression task. The ANOVA F-values help us assess the statistical significance of each feature's relationship with the target. By selecting the top 'k' features, we aim to keep the most informative features and reduce the model's complexity, which can help prevent overfitting."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DayOfWeek: The day of the week may impact sales due to different consumer behavior on different days.\n",
        "\n",
        "Customers: The number of customers visiting the store is a critical factor influencing sales.\n",
        "\n",
        "Promo: Whether there is a promotion on a particular day can significantly affect sales.\n",
        "\n",
        "SchoolHoliday: Sales may be influenced during school holidays when families have more time for shopping.\n",
        "\n",
        "StoreType: Different types of stores (A, B, C, D) may have varying sales patterns.\n",
        "\n",
        "Assortment: The assortment of products offered by the store could affect sales.\n",
        "\n",
        "CompetitionDistance: The distance to the nearest competitor's store may impact sales.\n",
        "\n",
        "PromoInterval_Feb,May,Aug,Nov: Promotion during specific months could have a positive impact on sales.\n",
        "\n",
        "PromoInterval_Jan,Apr,Jul,Oct: Promotion during specific months could also impact sales.\n",
        "\n",
        "PromoInterval_Mar,Jun,Sept,Dec: Promotion during specific months could have different effects on sales.\n",
        "\n",
        "Based on the ANOVA F-values, the top 10 features listed above are considered more important in explaining the variability in 'Sales'."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['Sales'] = np.log(merged_df['Sales'])\n",
        "merged_df.drop(merged_df[merged_df['Sales'] == float(\"-inf\")].index,inplace=True)"
      ],
      "metadata": {
        "id": "r5R5oNMpoRbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why We Used**-\n",
        "Log transformation is commonly used on the 'Sales' (or any positive numeric) variable when it exhibits positive skewness or a long tail to the right in its distribution. Positive skewness means that the data is skewed towards higher values, and the long tail indicates that there are extreme values that are much larger than the majority of the data."
      ],
      "metadata": {
        "id": "27809m_wpFz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "#no need here for data scaling as we already done some z- score and log transformation."
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "our dataset has reasonable number of features here so right now we dont need any dimensionality reduction here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "\n",
        "# Split the data into train and test sets\n",
        "#train_df, test_df = train_test_split(X,y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the sizes of the train and test sets\n",
        "#print(\"Train set size:\", len(train_df))\n",
        "#print(\"Test set size:\", len(test_df))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test_size parameter in the train_test_split function controls the proportion of the data that should be allocated to the testing set when splitting the dataset into training and testing sets. In the above code, test_size=0.2 is used, which means that 20% of the data will be allocated to the testing set, and the remaining 80% will be used for training.The commonly used splitting ratios are 80:20 (test_size=0.2) and 70:30 (test_size=0.3). These ratios strike a good balance between having enough data for training and obtaining a reliable evaluation on the testing set."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We will build four ML Models:\n",
        "\n",
        "\n",
        "*   Liniear Regression\n",
        "*   XGBoost\n",
        "*   Decision Tree\n",
        "*   Random Forest\n",
        "\n",
        "After building models we will evaluate their performance and select one which will give best results."
      ],
      "metadata": {
        "id": "efrb9Slpxfq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1-> Linear Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# Fitting Multiple Linear Regression to the Training set\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predicting test values:\n",
        "y_pred = regressor.predict(X_test)\n",
        "y_pred\n",
        "# Predict on the model\n",
        "# After building the model we are comparing the actual and the predicted values in this code:\n",
        "\n",
        "data = pd.DataFrame({'Actual':y_test, 'Predicted':y_pred})\n",
        "data"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Performance of the model\n",
        "\n",
        "r2s_1 = r2(y_test,y_pred)\n",
        "mae1 = mae(y_test,y_pred)\n",
        "rmse1 = math.sqrt(mse(y_test,y_pred))\n",
        "print('Performance of Linear Regression Model:')\n",
        "print('-'*40)\n",
        "print('r2_score:',r2s_1)\n",
        "print('Mean absolute error: %.2f' % mae1)\n",
        "print('Root mean squared error: ', rmse1)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Hyperparameter tuning for Ridge Regression\n",
        "ridge_params = {'alpha': [0.1, 1.0, 10.0]}\n",
        "ridge_model = Ridge()\n",
        "ridge_grid = GridSearchCV(ridge_model, ridge_params, cv=5, scoring='neg_mean_squared_error')\n",
        "ridge_grid.fit(X_train, y_train)\n",
        "\n",
        "# Hyperparameter tuning for Lasso Regression\n",
        "lasso_params = {'alpha': [0.1, 1.0, 10.0]}\n",
        "lasso_model = Lasso()\n",
        "lasso_grid = GridSearchCV(lasso_model, lasso_params, cv=5, scoring='neg_mean_squared_error')\n",
        "lasso_grid.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters for Ridge and Lasso Regression\n",
        "best_ridge_alpha = ridge_grid.best_params_['alpha']\n",
        "best_lasso_alpha = lasso_grid.best_params_['alpha']\n",
        "\n",
        "# Create Ridge and Lasso Regression models with the best hyperparameters\n",
        "best_ridge_model = Ridge(alpha=best_ridge_alpha)\n",
        "best_lasso_model = Lasso(alpha=best_lasso_alpha)\n",
        "\n",
        "# Fit the models on the training data\n",
        "best_ridge_model.fit(X_train, y_train)\n",
        "best_lasso_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "ridge_y_pred = best_ridge_model.predict(X_test)\n",
        "lasso_y_pred = best_lasso_model.predict(X_test)\n",
        "\n",
        "# Evaluate the models\n",
        "ridge_mse = mae(y_test, ridge_y_pred)\n",
        "ridge_r2 = r2(y_test, ridge_y_pred)\n",
        "\n",
        "lasso_mse = mae(y_test, lasso_y_pred)\n",
        "lasso_r2 = r2(y_test, lasso_y_pred)\n",
        "\n",
        "print(\"Ridge Regression:\")\n",
        "print(f\"Best alpha: {best_ridge_alpha}\")\n",
        "print(f\"MSE: {ridge_mse:.2f}\")\n",
        "print(f\"R-squared: {ridge_r2:.2f}\\n\")\n",
        "\n",
        "print(\"Lasso Regression:\")\n",
        "print(f\"Best alpha: {best_lasso_alpha}\")\n",
        "print(f\"MSE: {lasso_mse:.2f}\")\n",
        "print(f\"R-squared: {lasso_r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "uHfmdfWc3EOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " GridSearchCV is a technique that exhaustively searches for the best hyperparameters within a predefined set of hyperparameter values.\n",
        "\n",
        "The reason for choosing GridSearchCV is that it allows us to perform an exhaustive search over a specified range of hyperparameters. It evaluates the model's performance for each combination of hyperparameters using cross-validation and selects the best combination based on a specified scoring metric (in this case, we used negative mean squared error).\n",
        "\n",
        "By using GridSearchCV, we can ensure that we are exploring a range of hyperparameters thoroughly and selecting the best hyperparameters to build the most optimal model. GridSearchCV helps us avoid manually trying various combinations and automates the process of hyperparameter tuning, making it efficient and effective."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the evaluation metrics, we can observe that the R-squared value remains the same (around 0.87) for both Ridge and Lasso Regression models, with and without hyperparameter tuning. Additionally, the Mean Squared Error is also similar, with small differences (885.89 without tuning vs. 885.87 with tuning for Lasso).\n",
        "\n",
        "In this specific case, it seems that hyperparameter tuning did not lead to a significant improvement in model performance. However, it's important to note that these models are already performing quite well with an R-squared value of around 0.87, indicating a good fit to the data."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 -> XGBoost"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building XGBoost Regressor Model:\n",
        "\n",
        "xgboost = xgb.XGBRegressor(objective='reg:squarederror', verbosity=0)\n",
        "xgboost.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test data using the trained model\n",
        "y_pred = xgboost.predict(X_test)"
      ],
      "metadata": {
        "id": "A_dnk3up8KKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        " #Performance of the model\n",
        "r2 = r2(y_test, y_pred)\n",
        "mae = mae(y_test, y_pred)\n",
        "rmse = mse(y_test, y_pred, squared=False)\n",
        "\n",
        "# Printing the evaluation metrics\n",
        "print(\"Performance of XGBoost Regressor Model:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
        "print(f\"Root Mean Squared Error: {rmse:.2f}\")"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create the XGBoost regressor\n",
        "xgboost = xgb.XGBRegressor(objective='reg:linear', verbosity=0)\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "parameters = {'max_depth': [2, 5, 10],\n",
        "              'learning_rate': [0.05, 0.1, 0.2],\n",
        "              'min_child_weight': [1, 2, 5],\n",
        "              'gamma': [0, 0.1, 0.3],\n",
        "              'colsample_bytree': [0.3, 0.5, 0.7]}\n",
        "\n",
        "# RandomizedSearchCV for hyperparameter tuning with cross-validation\n",
        "xg_reg = RandomizedSearchCV(estimator=xgboost, param_distributions=parameters, n_iter=10, cv=3)\n",
        "xg_reg.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameter values and negative mean squared error\n",
        "print(\"The best parameters for XGBoost regression: \")\n",
        "for key, value in xg_reg.best_params_.items():\n",
        "    print(f\"{key}={value}\")\n",
        "print(f\"\\nNegative mean squared error: {xg_reg.best_score_}\")\n",
        "\n",
        "# Predict the test data\n",
        "y_test_pred = xg_reg.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "r2_score_test = r2(y_test, y_test_pred)\n",
        "mae_test = mae(y_test, y_test_pred)\n",
        "rmse_test = np.sqrt(mse(y_test, y_test_pred))\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"R-squared (Test): {r2_score_test:.4f}\")\n",
        "print(f\"Mean Absolute Error (Test): {mae_test:.2f}\")\n",
        "print(f\"Root Mean Squared Error (Test): {rmse_test:.2f}\")\n"
      ],
      "metadata": {
        "id": "1rtdGu9GjOPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i have used here randomizedCV because of certain reason-\n",
        "Faster Computation: RandomizedSearchCV samples a fixed number of hyperparameter combinations randomly, whereas GridSearchCV exhaustively searches through all possible combinations. As a result, RandomizedSearchCV is generally faster because it evaluates fewer parameter settings.\n",
        "\n",
        "Flexibility: RandomizedSearchCV allows you to specify the number of iterations (n_iter) instead of specifying a specific grid of hyperparameter values. This makes it more flexible when you have a large hyperparameter space and want to explore a random subset of it.\n",
        "\n",
        "Better Exploration: RandomizedSearchCV tends to explore a broader range of hyperparameter values. This can be beneficial when the best hyperparameters are not located on the grid points of the traditional grid search.\n",
        "\n",
        "Resource-Efficient: When dealing with computationally expensive models and large datasets, RandomizedSearchCV can be more resource-efficient since it runs fewer iterations compared to GridSearchCV."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the model's performance has improved after hyperparameter tuning, as indicated by higher R-squared and lower MAE and RMSE values. This suggests that the tuned XGBoost model is better at capturing the relationships between the features and the target variable, resulting in more accurate predictions."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R-squared (R2 score):\n",
        "\n",
        "Indication: R-squared represents the proportion of the variance in the target variable (sales in this case) that is explained by the independent variables (features) in the model. A higher R-squared value indicates that the model explains a larger percentage of the variance, which means it fits the data well and captures the relationship between the input features and the target.\n",
        "\n",
        "Business Impact: A high R-squared value implies that the model is accurately predicting sales based on the available features. This means that the model's predictions are closer to the actual sales values, making it more reliable for making business decisions. A good R-squared score indicates that the model can provide valuable insights into the factors affecting sales, which can lead to better resource allocation, inventory management, and promotion planning, ultimately optimizing the retail business's operations and maximizing profitability.\n",
        "\n",
        "Mean Absolute Error (MAE):\n",
        "\n",
        "Indication: MAE measures the average absolute difference between the actual sales values and the predicted values by the model. A lower MAE indicates that the model's predictions are closer to the actual sales values, meaning it has lower prediction errors.\n",
        "\n",
        "Business Impact: A lower MAE signifies that the model's predictions are more accurate and closer to the actual sales figures. This accuracy is crucial for retail businesses to plan their operations and inventory management more efficiently. When the model's predictions are accurate, it helps in avoiding overstocking or understocking of products, leading to reduced inventory holding costs and improved customer satisfaction. It also aids in making more informed decisions on pricing and promotions to boost sales.\n",
        "\n",
        "Root Mean Squared Error (RMSE):\n",
        "\n",
        "Indication: RMSE measures the square root of the average of the squared differences between the actual sales values and the predicted values. Like MAE, a lower RMSE indicates that the model's predictions are closer to the actual values, but it is more sensitive to larger prediction errors.\n",
        "\n",
        "Business Impact: A lower RMSE indicates that the model is making more precise predictions, which is essential for minimizing forecasting errors in demand and sales. Minimizing RMSE helps in optimizing inventory levels, ensuring products are available when needed, and avoiding excess inventory that may lead to potential losses. Additionally, accurate predictions aid in better resource planning, cost optimization, and identifying sales opportunities during peak seasons."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3-> Decision Tree"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# Create a Decision Tree Regressor\n",
        "decision_tree_model = DecisionTreeRegressor()\n",
        "# Fit the Algorithm\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "# Predict on the model\n",
        "y_test_pred = decision_tree_model.predict(X_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculate evaluation metrics\n",
        "r2_score_test1 = r2(y_test, y_test_pred)\n",
        "mae_test1 = mae(y_test, y_test_pred)\n",
        "rmse_test1 = mse(y_test, y_test_pred, squared=False)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Performance of Decision Tree Regressor Model:\")\n",
        "print(\"-------------------------------------------\")\n",
        "print(f\"R-squared (Test): {r2_score_test1:.4f}\")\n",
        "print(f\"Mean Absolute Error (Test): {mae_test1:.2f}\")\n",
        "print(f\"Root Mean Squared Error (Test): {rmse_test1:.2f}\")"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Create a Decision Tree Regressor model\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [None, 5, 10, 15, 20],\n",
        "    'min_samples_split': [2, 5, 10, 15, 20],\n",
        "    'min_samples_leaf': [1, 2, 4, 8, 12],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search_cv = GridSearchCV(estimator=dt_regressor, param_grid=param_grid, cv=3)\n",
        "# Fit the Algorithm\n",
        "grid_search_cv.fit(X_train, y_train)\n",
        "# Predict on the model\n",
        "# Predict on test data\n",
        "y_test_pred_grid = grid_search_cv.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "r2_score_test_grid = r2(y_test, y_test_pred_grid)\n",
        "mae_test_grid = mae(y_test, y_test_pred_grid)\n",
        "rmse_test_grid = np.sqrt(mse(y_test, y_test_pred_grid))\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Performance of Decision Tree Regressor Model (with GridSearchCV):\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"R-squared (Test): {r2_score_test_grid:.4f}\")\n",
        "print(f\"Mean Absolute Error (Test): {mae_test_grid:.2f}\")\n",
        "print(f\"Root Mean Squared Error (Test): {rmse_test_grid:.2f}\")"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is used for hyperparameter tuning in this case because it performs an exhaustive search over a specified hyperparameter grid, trying all possible combinations of hyperparameters. Since Decision Tree Regressor has several hyperparameters that can significantly impact the model's performance, it is essential to find the best combination of hyperparameters for the model."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improvements:\n",
        "\n",
        "R-squared (Test) improved from 0.9655 to 0.9743, indicating that the model with hyperparameter tuning explains more variance in the test data, making it a better fit for the data.\n",
        "\n",
        "Mean Absolute Error (Test) reduced from 401.43 to 344.03, showing that the model with hyperparameter tuning has better accuracy in predicting the sales values, resulting in smaller absolute errors between actual and predicted sales.\n",
        "\n",
        "Root Mean Squared Error (Test) decreased from 633.21 to 546.44, indicating that the model with hyperparameter tuning has better precision in predicting the sales values, resulting in smaller errors on average.\n",
        "\n",
        "Overall, the model with GridSearchCV hyperparameter tuning outperforms the model without hyperparameter tuning in terms of predictive performance and accuracy, making it a better choice for the given dataset."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4-> Random Forest"
      ],
      "metadata": {
        "id": "Z9h2BcnqCJfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Initialize the Random Forest Regressor\n",
        "random_forest = RandomForestRegressor(random_state=42)\n",
        "# Fit the Algorithm\n",
        "random_forest.fit(X_train, y_train)\n",
        "# Predict on the model\n",
        "# Make predictions on the test data\n",
        "y_test_pred = random_forest.predict(X_test)"
      ],
      "metadata": {
        "id": "wlYGiSL2CJfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ROoqMKeGCJfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Evaluate the model's performance\n",
        "r2_score_test2 = r2(y_test, y_test_pred)\n",
        "mae_test2 = mae(y_test, y_test_pred)\n",
        "rmse_test2 = np.sqrt(mse(y_test, y_test_pred))\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Performance of Random Forest Regressor Model:\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"R-squared (Test): {r2_score_test2:.4f}\")\n",
        "print(f\"Mean Absolute Error (Test): {mae_test2:.2f}\")\n",
        "print(f\"Root Mean Squared Error (Test): {rmse_test2:.2f}\")"
      ],
      "metadata": {
        "id": "-oCjF7-rCJfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "EoO1Q_hMCJfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Create a Random Forest Regressor model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_features': ['auto', 'sqrt'],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Create RandomizedSearchCV object\n",
        "randomized_search = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=5, cv=3, random_state=42)\n",
        "# Fit the Algorithm\n",
        "# Fit the model on the training data\n",
        "randomized_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best estimator from the RandomizedSearchCV\n",
        "best_params = randomized_search.best_params_\n",
        "best_rf_model = randomized_search.best_estimator_\n",
        "\n",
        "# Predict on the test data\n",
        "y_test_pred_rf = best_rf_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "r2_score_rf = r2(y_test, y_test_pred_rf)\n",
        "mae_rf = mae(y_test, y_test_pred_rf)\n",
        "rmse_rf = np.sqrt(mse(y_test, y_test_pred_rf))\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Performance of Random Forest Regressor Model (with RandomizedSearchCV):\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"R-squared (Test): {r2_score_rf:.4f}\")\n",
        "print(f\"Mean Absolute Error (Test): {mae_rf:.2f}\")\n",
        "print(f\"Root Mean Squared Error (Test): {rmse_rf:.2f}\")\n"
      ],
      "metadata": {
        "id": "UVn1QeixTGYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "bE-gLbfGCJfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Random Forest model, I used the RandomizedSearchCV hyperparameter optimization technique. The RandomizedSearchCV method randomly samples a subset of the hyperparameter space and evaluates the model's performance for each combination. This technique is more efficient and faster compared to GridSearchCV because it explores only a limited number of hyperparameter combinations, which is especially useful when dealing with large datasets or a high number of hyperparameters.\n",
        "\n",
        "The main advantage of RandomizedSearchCV is that it can efficiently find good hyperparameter configurations without exhaustively searching through all possible combinations. It allows us to specify the number of iterations (n_iter) to control the number of parameter settings that are sampled."
      ],
      "metadata": {
        "id": "j3DFnfQGCJfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "GoguBvA0CJfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, after hyperparameter tuning, the R-squared (Test) increased from 0.9784 to 0.9795, indicating a slight improvement in the model's ability to explain the variance in the test data. Additionally, the Mean Absolute Error (MAE) decreased from 322.86 to 314.08, and the Root Mean Squared Error (RMSE) decreased from 500.91 to 488.38. Both of these metrics suggest that the model's predictions are closer to the actual values, indicating improved accuracy and precision.\n",
        "\n",
        "Overall, the Random Forest model with hyperparameter tuning performed slightly better than the model without tuning, leading to a more accurate and reliable predictive performance."
      ],
      "metadata": {
        "id": "JJUkr_2uCJfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a positive business impact, we consider the following evaluation metrics for the Decision Tree model:\n",
        "\n",
        "R-squared (Coefficient of Determination): R-squared represents the proportion of variance in the dependent variable (sales) that is predictable from the independent variables (features). A higher R-squared value indicates that the model explains more variance in the sales data, which means it can better capture the underlying patterns and trends. This is important for businesses as it helps in understanding how well the model fits the data and how effective it is in predicting sales.\n",
        "\n",
        "Mean Absolute Error (MAE): MAE measures the average absolute difference between the actual sales values and the predicted sales values. A lower MAE indicates that the model has better accuracy in predicting sales, as it has smaller absolute errors between actual and predicted values. This is crucial for businesses because accurate sales predictions help in better resource allocation, inventory management, and decision-making.\n",
        "\n",
        "Root Mean Squared Error (RMSE): RMSE is the square root of the average of the squared differences between the actual sales values and the predicted sales values. It provides a measure of the precision of the model's predictions. A lower RMSE means the model's predictions are closer to the actual values on average, making it more reliable for businesses to make data-driven decisions."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the evaluation metric scores and the comparison of different models, the final prediction model I would choose is the Random Forest Regressor model with hyperparameter tuning using RandomizedSearchCV. This model achieved the following evaluation metric scores on the test data:\n",
        "\n",
        "**`R-squared (Test)`**: 0.9795\n",
        "\n",
        "**`Mean Absolute Error (Test)`**: 314.08\n",
        "\n",
        "**`Root Mean Squared Error (Test)`**: 488.38\n",
        "\n",
        "The Random Forest model with hyperparameter tuning outperformed the other models, including Linear Regression, XGBoost, and Decision Tree models, in terms of R-squared, MAE, and RMSE. It demonstrated the highest R-squared value, indicating a better ability to explain the variance in the test data. Additionally, it had the lowest MAE and RMSE values, suggesting higher accuracy and precision in its predictions.\n",
        "\n",
        "Furthermore, Random Forest models are known for their ability to handle non-linear relationships, interactions among features, and robustness to overfitting. By tuning the hyperparameters using RandomizedSearchCV, we were able to find the optimal combination of hyperparameters that improved the model's performance.\n",
        "\n",
        "Therefore, considering its superior performance in evaluation metrics and its robustness in handling complex data, the Random Forest Regressor model with hyperparameter tuning is chosen as the final prediction model for this dataset."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model used for the final prediction is the Random Forest Regressor with hyperparameter tuning using RandomizedSearchCV. Random Forest is an ensemble learning method that builds multiple decision trees during training and outputs the mean prediction of the individual trees. It is an extension of the Decision Tree algorithm and is known for its ability to handle complex data, handle non-linear relationships, and avoid overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Explainability Tool**:-\n",
        "\n",
        "One popular model explainability tool is SHAP (SHapley Additive exPlanations). SHAP values provide a unified measure of feature importance, showing the contribution of each feature to each prediction."
      ],
      "metadata": {
        "id": "WKxtSx1qjmFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap==0.42.1"
      ],
      "metadata": {
        "id": "at__98R7upWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "# Create a Tree Explainer object for the trained Random Forest model\n",
        "explainer = shap.TreeExplainer(best_rf_model)\n",
        "\n",
        "# Calculate SHAP values for the test data\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Plot the SHAP summary plot to visualize feature importance\n",
        "shap.summary_plot(shap_values, X_test)"
      ],
      "metadata": {
        "id": "orEuVLv-ltLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Businesses use sales forecasts to determine what revenue they will be generating in a particular timespan to empower themselves with powerful and strategic business plans. Important decisions such as budgets, hiring, incentives, goals, acquisitions and various other growth plans are affected by the revenue the company is going to make in the coming months and for these plans to be as effective as they are planned to be it is important for these forecasts to also be as good.\n",
        "\n",
        "The work here forecasts the sales of the various Rossmann stores across Europe for the recent six weeks and compares the results from the models developed with the actual sales values.\n",
        "\n",
        "Some important conclusions drawn from the analysis are as follows:\n",
        "* there were more sales on Monday, probably because shops generally remain closed on Sundays which had the lowest sales in a week. This validates the hypothesis about this feature.\n",
        "* The positive effect of promotion on Customers and Sales is observable.\n",
        "* Most stores have competition distance within the range of 0 to 10 kms and had more sales than stores far away probably indicating competition in busy locations vs remote locations.\n",
        "* Store type B though being few in number had the highest sales average. The reasons include all three kinds of assortments specially assortment level b which is only available at type b stores and being open on sundays as well.\n",
        "* The outliers in the dataset showed justifiable behaviour. The outliers were either of store type b or had promotion going on which increased sales.\n",
        "* Out of the four methods, Random Forest proved to be the most accurate, achieving a R2_Score of 0.9795, MAE of 314.08 and RMSE of 488.38. While it has the lowest error of all methods, it requires more work than the other three approaches and hence, comsumes more time to produce results.\n",
        "\n",
        "From results, we can see that most important feature on which sale of store depends is 'Customers'. Also, the customer feature seems to depend on other features like Competition distance, Store type , Promo etc.\n",
        "\n",
        "So, now we can say that the Rossmann store person can now implement the Random Forest Model and utilise the feature importance data for predicting the sales for next six months.\n",
        "\n",
        "The results of all predictions may be skewed due to data preprocessing, as the training set contains a large portion of incomplete entries that had to be filled with most fitting values."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}